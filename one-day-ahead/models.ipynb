{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":12345077,"datasetId":7544216,"databundleVersionId":12902669},{"sourceType":"datasetVersion","sourceId":11794887,"datasetId":7406505,"databundleVersionId":12283169}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 5.5) Gathering Input Graphs","metadata":{}},{"cell_type":"code","source":"!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n!pip install torch-geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:30:24.027128Z","iopub.execute_input":"2025-07-02T09:30:24.027316Z","iopub.status.idle":"2025-07-02T09:30:36.464404Z","shell.execute_reply.started":"2025-07-02T09:30:24.027299Z","shell.execute_reply":"2025-07-02T09:30:36.463548Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.2+pt25cu124\nLooking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\nCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2024.2.0)\nInstalling collected packages: torch-sparse\nSuccessfully installed torch-sparse-0.6.18+pt25cu124\nCollecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.18)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch_geometric.data import Data\nfrom torch.serialization import add_safe_globals\nimport os\n\n# Allow PyTorch to unpickle torch_geometric.data.Data objects\nadd_safe_globals([Data])\n\n# Then load the files\ngraph_dir = \"/kaggle/input/chicken-productivity-rate-graphs\"\npt_files = [f for f in os.listdir(graph_dir) if f.endswith('.pt')]\n\ngraphs = [\n    torch.load(os.path.join(graph_dir, f), weights_only=False)\n    for f in pt_files\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:31:59.464904Z","iopub.execute_input":"2025-07-02T09:31:59.465199Z","iopub.status.idle":"2025-07-02T09:32:15.076701Z","shell.execute_reply.started":"2025-07-02T09:31:59.465170Z","shell.execute_reply":"2025-07-02T09:32:15.076156Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZN2at4_ops16div__Tensor_mode4callERNS_6TensorERKS2_St8optionalIN3c1017basic_string_viewIcEEE\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_spmm_cuda.so: undefined symbol: _ZN5torch8autograd12VariableInfoC1ERKN2at6TensorE\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nrates = []\nfor graph in graphs:\n    if hasattr(graph, 'y'):\n        val = graph.y.item() if graph.y.numel() == 1 else float(graph.y[0])\n        rates.append(val)\n\nplt.figure(figsize=(10, 6))\nplt.hist(rates, bins=15, edgecolor='black')\nplt.xlabel(\"Productivity Rate\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Productivity Rate Across Graph Dataset\")\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:32:32.018598Z","iopub.execute_input":"2025-07-02T09:32:32.019412Z","iopub.status.idle":"2025-07-02T09:32:32.511373Z","shell.execute_reply.started":"2025-07-02T09:32:32.019388Z","shell.execute_reply":"2025-07-02T09:32:32.510774Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABYM0lEQVR4nO3deZyNdf/H8fc5ZjVjjBmzGMYue9Rosa+FkH3LOkh3UbbSrW5lKVJ3lkpUt4YsxSiKbk1jiUhiisiWwogxDMbMYBZzrt8ffnNuxwzGmMsx0+v5eMyjru91nev6XOf6nst5n2uzGIZhCAAAAAAA5DurswsAAAAAAKCwInQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMoFCZMmCCLxXJHltWsWTM1a9bMPvzdd9/JYrFo+fLld2T5AwcOVPny5e/IsvIqJSVFQ4YMUXBwsCwWi0aOHOnskq7rTvada13bl3Irq8999913+V4TkF+aNWumWrVqObsMAHA6QjeAu878+fNlsVjsfx4eHgoJCVHr1q31zjvvKDk5OV+Wc+LECU2YMEE7d+7Ml/nlp7u5ttyYMmWK5s+fr6effloLFy5Uv379rjtt+fLlHbZ3YGCgGjdurBUrVtzBis2zd+9eTZgwQUeOHDF1OUuWLNHMmTPzfb5XbxuLxSIfHx81bdpUX3/9dZ7naVatV+vRo4csFotefPFFU5fjbGlpaXr33XfVqFEjlShRQm5ubgoJCdHjjz+uTz/9VJmZmc4uMc+u3jdYrVb5+vqqdu3aGjp0qLZt23Zb854yZYpWrlyZP4Xepju1jwDgPBbDMAxnFwEAV5s/f77Cw8M1adIkVahQQRkZGTp58qS+++47RUdHq2zZsvrqq69077332l9z+fJlXb58WR4eHrlezo4dO/TAAw8oIiJCAwcOzPXr0tPTJUlubm6Srhx1bN68uSIjI9WtW7dczyevtWVkZMhms8nd3T1flmWGhx9+WC4uLtq8efNNpy1fvrxKlCihMWPGSLryg8MHH3ygP//8U3PmzNE//vEPU2udMGGCJk6cKLP+OVy+fLm6d++uDRs2ZDuqfW1fyi2bzab09HS5ubnJar3y+3n79u21Z8+efP/ibrFY9Mgjj6h///4yDENHjx7VnDlzFBcXpzVr1qh169a3PE+zas2SlJSkoKAgBQcHKzMzU0ePHnXa2QxmOn36tNq2bauYmBi1bt1ajzzyiPz8/HTy5EmtXbtW69ev16RJkzR+/Hin1NesWTMlJCRoz549eXr9tfuG5ORk7du3T5GRkTp58qRGjRql6dOn52ne3t7e6tatm+bPn5+n1+enG+0jABQOLs4uAACup23btqpXr559eNy4cVq/fr3at2+vxx9/XPv27ZOnp6ckycXFRS4u5u7SLl68qKJFi95yQMpvrq6uTl1+bpw6dUo1atTI9fSlS5dW37597cP9+/dX5cqVNWPGjOuG7suXL8tmszl9e9yOvNZutVpv6Qem23XPPfc4bJ+uXbuqRo0amjVrVp5Ct9k+//xzZWZm6uOPP1aLFi20adMmNW3aNF/mfeHCBXl5eeXLvG5Xv3799Msvv+jzzz9Xly5dHMaNGzdOO3bs0IEDB244j9TUVIcfb+421+4bJGnatGl64oknNGPGDFWpUkVPP/20k6oDgNy5O/ewAHAdLVq00Pjx43X06FEtWrTI3p7TdbnR0dFq1KiRfH195e3trapVq+qll16SdOXo9AMPPCBJCg8Pt5/CmHXUI+taxJiYGDVp0kRFixa1v/Z61+FmZmbqpZdeUnBwsLy8vPT444/r2LFjDtOUL18+x6PqV8/zZrXldE33hQsXNGbMGIWGhsrd3V1Vq1bVv//972xHby0Wi4YPH66VK1eqVq1acnd3V82aNfXNN9/k/IZf49SpUxo8eLCCgoLk4eGhOnXqaMGCBfbxWdcaHz58WF9//bW99ls9ohkcHKzq1avr8OHDkqQjR47IYrHo3//+t2bOnKlKlSrJ3d1de/fulSStX79ejRs3lpeXl3x9fdWxY0ft27cv23w3b96sBx54QB4eHqpUqZI++OCDbNNkLSunI2AWi0UTJkxwaDt+/LgGDx6skJAQubu7q0KFCnr66aeVnp6u+fPnq3v37pKk5s2b29+PrGuxr97u8fHxcnFx0cSJE7Mt98CBA7JYLHrvvfckZb+mu1mzZvr666/tR3QtFovKly+vlJQUeXl5acSIEdnm+ddff6lIkSKaOnVq9g1wE9WrV1fJkiX1xx9/OLR/+eWXateunf29qFSpkiZPnuxwivP1as2SlpamV199VZUrV5a7u7tCQ0M1duxYpaWl5bq+xYsX65FHHlHz5s1VvXp1LV68OMfp9u/frx49eiggIECenp6qWrWqXn75Zfv4rP3K3r179cQTT6hEiRJq1KiRpCs/+kyePNneF8uXL6+XXnopW507duxQ69atVbJkSXl6eqpChQoaNGiQwzSfffaZwsLCVKxYMfn4+Kh27dqaNWvWDddx69atioqK0tChQ7MF7iz16tVTnz597MNZ/eazzz7Tv/71L5UuXVpFixZVUlKSzp49q+eff161a9eWt7e3fHx81LZtW+3atcthnlnzWLp06U33d1n27t2r5s2bq2jRoipdurTefPPNG67bzXh6emrhwoXy8/PT66+/7rCf+/e//60GDRrI399fnp6eCgsLy3a/DYvFogsXLmjBggX2Ppi1Xz569KieeeYZVa1aVZ6envL391f37t2z7cMyMjI0ceJEValSRR4eHvL391ejRo0UHR3tMN3+/fvVrVs3+fn5ycPDQ/Xq1dNXX31lH3+zfQSAwoEj3QAKnH79+umll17St99+qyeffDLHaX777Te1b99e9957ryZNmiR3d3cdOnRIW7ZskXQlNEyaNEmvvPKKhg4dqsaNG0uSGjRoYJ/HmTNn1LZtW/Xq1Ut9+/ZVUFDQDet6/fXX7deQnjp1SjNnzlSrVq20c+dO+xH53MhNbVczDEOPP/64NmzYoMGDB6tu3bqKiorSCy+8oOPHj2vGjBkO02/evFlffPGFnnnmGRUrVkzvvPOOunbtqtjYWPn7+1+3rkuXLqlZs2Y6dOiQhg8frgoVKigyMlIDBw5UYmKiRowYoerVq2vhwoUaNWqUypQpYz8tNCAgINfrL135Qnvs2LFs9URERCg1NVVDhw6Vu7u7/Pz8tHbtWrVt21YVK1bUhAkTdOnSJb377rtq2LChfv75Z3ug2717tx599FEFBARowoQJunz5sl599dWbbtcbOXHihB588EElJiZq6NChqlatmo4fP67ly5fr4sWLatKkiZ577jm98847eumll1S9enVJsv/3akFBQWratKmWLVumV1991WHc0qVLVaRIEfuX82u9/PLLOn/+vP766y/79vb29pa3t7c6d+6spUuXavr06SpSpIj9NZ9++qkMw3AIZbl1/vx5nTt3TpUqVXJonz9/vry9vTV69Gh5e3tr/fr1euWVV5SUlKS33nrrhrVKV06bf/zxx7V582YNHTpU1atX1+7duzVjxgwdPHgwV9fgnjhxQhs2bLD/GNS7d2/NmDFD7733nsOZBb/++qsaN24sV1dXDR06VOXLl9cff/yhVatW6fXXX3eYZ/fu3VWlShVNmTLFHvCGDBmiBQsWqFu3bhozZoy2bdumqVOnat++ffb7EZw6dcre5/75z3/K19dXR44c0RdffGGfd3R0tHr37q2WLVtq2rRpkqR9+/Zpy5YtOf5YkmXVqlWSlO0ocG5MnjxZbm5uev7555WWliY3Nzft3btXK1euVPfu3VWhQgXFx8frgw8+UNOmTbV3716FhIQ4zCO3+7tz586pTZs26tKli3r06KHly5frxRdfVO3atdW2bdtbrj1LVt+eN2+e9u7dq5o1a0qSZs2apccff1x9+vRRenq6PvvsM3Xv3l2rV69Wu3btJEkLFy7UkCFD9OCDD2ro0KGSZO/L27dv1w8//KBevXqpTJkyOnLkiObMmaNmzZpp7969Klq0qKQrP8hMnTrVPp+kpCTt2LFDP//8sx555BFJV/4datiwoUqXLq1//vOf8vLy0rJly9SpUyd9/vnn6ty58y3tIwAUYAYA3GUiIiIMScb27duvO03x4sWN++67zz786quvGlfv0mbMmGFIMk6fPn3deWzfvt2QZERERGQb17RpU0OSMXfu3BzHNW3a1D68YcMGQ5JRunRpIykpyd6+bNkyQ5Ixa9Yse1u5cuWMAQMG3HSeN6ptwIABRrly5ezDK1euNCQZr732msN03bp1MywWi3Ho0CF7myTDzc3NoW3Xrl2GJOPdd9/NtqyrzZw505BkLFq0yN6Wnp5u1K9f3/D29nZY93Llyhnt2rW74fyunvbRRx81Tp8+bZw+fdrYtWuX0atXL0OS8eyzzxqGYRiHDx82JBk+Pj7GqVOnHF5ft25dIzAw0Dhz5ozDOlmtVqN///72tk6dOhkeHh7G0aNH7W179+41ihQp4tB3spaV03svyXj11Vftw/379zesVmuOfdVmsxmGYRiRkZGGJGPDhg3Zprl2u3/wwQeGJGP37t0O09WoUcNo0aKFfTirz109z3bt2jn0iyxRUVGGJGPNmjUO7ffee6/Dsq9HkjF48GDj9OnTxqlTp4wdO3YYbdq0MSQZb731lsO0Fy9ezPb6p556yihatKiRmpp601oXLlxoWK1W4/vvv3donzt3riHJ2LJly03r/fe//214enra++PBgwcNScaKFSscpmvSpIlRrFgxh/5gGP/bbobxv/1K7969HabZuXOnIckYMmSIQ/vzzz9vSDLWr19vGIZhrFix4qb7shEjRhg+Pj7G5cuXb7puV+vcubMhyUhMTHRov3Tpkv2zdPr0aePcuXP2cVn9pmLFitm2VWpqqpGZmenQdvjwYcPd3d2YNGlStnnkZn+XtR/95JNP7G1paWlGcHCw0bVr15uu4832I1n7+S+//NLedu16paenG7Vq1XL4/BiGYXh5eeW4L86pD2/dujXbetSpU+em+7iWLVsatWvXduj7NpvNaNCggVGlShV72432EQAKB04vB1AgeXt73/Au5r6+vpKunO5qs9nytAx3d3eFh4fnevr+/furWLFi9uFu3bqpVKlS+u9//5un5efWf//7XxUpUkTPPfecQ/uYMWNkGIbWrFnj0N6qVSuHI5T33nuvfHx89Oeff950OcHBwerdu7e9zdXVVc8995xSUlK0cePGPK/Dt99+q4CAAAUEBKhOnTqKjIxUv3797Ef+snTt2tXhqHlcXJx27typgQMHys/Pz2GdHnnkEft7n5mZqaioKHXq1Elly5a1T1e9evU8X5Nss9m0cuVKdejQweHeA1nycuOuLl26yMXFRUuXLrW37dmzR3v37lXPnj3zVGerVq0UEhLicIr1nj179Ouvv+b6KOm8efMUEBCgwMBA1atXT+vWrdPYsWM1evRoh+muPsKZnJyshIQENW7cWBcvXtT+/ftvupzIyEhVr15d1apVU0JCgv2vRYsWkqQNGzbcdB6LFy9Wu3bt7J/FKlWqKCwszGH9T58+rU2bNmnQoEEO/UHKebtde1+BrH517fpnndmRdWf3rP3Q6tWrlZGRkWO9vr6+unDhQrbTkm8mKSlJ0v/OEsgyd+5c+2cpICDAfjr81QYMGJDt7Bt3d3f7dd2ZmZk6c+aM/bKcn3/+Ods8cru/8/b2duhnbm5uevDBB2+6v8mNrHW/+t+Ca4+ynz9/Xo0bN85xHXJy9eszMjJ05swZVa5cWb6+vg7z8PX11W+//abff/89x/mcPXtW69evV48ePeyfhYSEBJ05c0atW7fW77//ruPHj9/S+gIouAjdAAqklJQUhy981+rZs6caNmyoIUOGKCgoSL169dKyZctuKYCXLl36lm50VaVKFYdhi8WiypUrm/4YmKNHjyokJCTb+5F1euLRo0cd2q8NGZJUokQJnTt37qbLqVKlSrYbLl1vObfioYceUnR0tNauXasffvhBCQkJ+uSTT7IFgwoVKmSrSZKqVq2abZ7Vq1dXQkKCLly4oNOnT+vSpUvZttH1Xpsbp0+fVlJSUr4+h7hkyZJq2bKlli1bZm9bunSpXFxcrnvd7s1YrVb16dNHK1eu1MWLFyVdCaYeHh7XPV39Wh07dlR0dLS+/vpr+3XOFy9ezNYXfvvtN3Xu3FnFixeXj4+PAgIC7IHr/PnzN13O77//rt9++80hNAYEBOiee+6RdOV07RvZt2+ffvnlFzVs2FCHDh2y/zVr1kyrV6+2B9WswJfbbZdTv7NarapcubJDe3BwsHx9fe39smnTpuratasmTpyokiVLqmPHjoqIiHC47vuZZ57RPffco7Zt26pMmTIaNGhQru6xkPV5T0lJcWjv2rWroqOjFR0d7fCEhxutj3TlR6SsG5O5u7urZMmSCggI0K+//prjtsvt/q5MmTLZfsjIzf4mN7LW/ep93+rVq/Xwww/Lw8NDfn5+CggI0Jw5c3LV/6Qrl9G88sor9vtjZL0PiYmJDvOYNGmSEhMTdc8996h27dp64YUX9Ouvv9rHHzp0SIZhaPz48dn6c9blIzfrzwAKD67pBlDg/PXXXzp//ny2L7xX8/T01KZNm7RhwwZ9/fXX+uabb7R06VK1aNFC3377rcO1rTeaR3673tHPzMzMXNWUH663HMOJT5AsWbKkWrVqddPpzNgm17rRNroTevXqpfDwcO3cuVN169bVsmXL1LJlS5UsWTLP8+zfv7/eeustrVy5Ur1799aSJUvUvn17FS9ePFevL1OmjH37PPbYYypZsqSGDx+u5s2b238MSExMVNOmTeXj46NJkyapUqVK8vDw0M8//6wXX3wxVz942Ww21a5d+7qPgQoNDb3h67Nurjhq1CiNGjUq2/jPP//8ls5eyXK9fnezsxksFouWL1+uH3/8UatWrVJUVJQGDRqkt99+Wz/++KO8vb0VGBionTt3KioqSmvWrNGaNWsUERGh/v37O9yk8FrVqlWTdOWshYYNG9rbQ0ND7e9TiRIllJCQkKv1mTJlisaPH69BgwZp8uTJ8vPzk9Vq1ciRI/N8tpBk7v4m61FkWf8WfP/993r88cfVpEkTvf/++ypVqpRcXV0VERGhJUuW5Gqezz77rCIiIjRy5EjVr19fxYsXl8ViUa9evRzehyZNmuiPP/7Ql19+qW+//Vb/+c9/NGPGDM2dO1dDhgyxT/v8889f92yaG/0bBqBwIXQDKHAWLlwoSTc9Ldhqtaply5Zq2bKlpk+frilTpujll1/Whg0b1KpVq3x/bu+1pxkahqFDhw45HG0qUaKEEhMTs7326NGjqlixon34VmorV66c1q5dq+TkZIcjPlmn85YrVy7X87rZcn799VfZbDaHI5z5vZxbrUlSjo9F2r9/v0qWLCkvLy95eHjI09Mzx1NBr31tiRIlJCnbdrr2SH5AQIB8fHxu+gziW+1nnTp10lNPPWU/xfzgwYMaN27cTV93o+XUqlVL9913nxYvXqwyZcooNjZW77777i3VdbWnnnpKM2bM0L/+9S917tzZfrflM2fO6IsvvlCTJk3s02bdgT43tVaqVEm7du1Sy5Ytb/l9MwxDS5YsUfPmzfXMM89kGz958mQtXrxY4eHh9s9aXp8fXa5cOdlsNv3+++8ON7yKj49XYmJits/Cww8/rIcfflivv/66lixZoj59+uizzz7TkCFDJF055bpDhw7q0KGDbDabnnnmGX3wwQcaP378dYNZ+/bt9cYbb2jx4sUOoTuvli9frubNm2vevHkO7YmJiTn+4JOb/Z2ZUlJStGLFCoWGhtq3weeffy4PDw9FRUXJ3d3dPm1ERES211+vfy1fvlwDBgzQ22+/bW9LTU3Ncb/t5+en8PBwhYeHKyUlRU2aNNGECRM0ZMgQex9zdXW96Q+KhfEZ8gAccXo5gAJl/fr1mjx5sipUqHDDuy6fPXs2W1vdunUlyX5qZ9azdnP6MpUXn3zyicO1hcuXL1dcXJzDHXorVaqkH3/8Uenp6fa21atXZ3vUzq3U9thjjykzM9P+OKksM2bMkMViua07BF+7nJMnTzpcb3z58mW9++678vb2zrfnIN+KUqVKqW7dulqwYIHDe7Vnzx59++23euyxxyRdOdrWunVrrVy5UrGxsfbp9u3bp6ioKId5+vj4qGTJktq0aZND+/vvv+8wbLVa1alTJ61atUo7duzIVlvWkbxb7We+vr5q3bq1li1bps8++0xubm7q1KnTTV/n5eV1w1No+/Xrp2+//VYzZ86Uv7//bfULFxcXjRkzRvv27dOXX34p6X9HNK8+gpmenp7tfbtRrT169NDx48f10UcfZRt36dIlXbhw4bo1bdmyRUeOHFF4eLi6deuW7a9nz57asGGDTpw4oYCAADVp0kQff/yxQ3+4tv7ryepXM2fOdGjPOkKfdZfsc+fOZZvftfuhM2fOOIy3Wq324Hqjx6Q1bNhQjzzyiD788EP7NrjWrRxNLlKkSLbpIyMjr3vdcW72d2a5dOmS+vXrp7Nnz+rll1+2h9YiRYrIYrE4nJVy5MiRHO967+XlleNnMqf34d133812psu1283b21uVK1e2b7PAwEA1a9ZMH3zwgeLi4rIt5/Tp0w61SPn3bxGAuw9HugHctdasWaP9+/fr8uXLio+P1/r16xUdHa1y5crpq6++koeHx3VfO2nSJG3atEnt2rVTuXLldOrUKb3//vsqU6aM/cZClSpVkq+vr+bOnatixYrJy8tLDz30UI7XO+aGn5+fGjVqpPDwcMXHx2vmzJmqXLmyw2PNhgwZouXLl6tNmzbq0aOH/vjjDy1atCjbo5dupbYOHTqoefPmevnll3XkyBHVqVNH3377rb788kuNHDky27zzaujQofrggw80cOBAxcTEqHz58lq+fLm2bNmimTNn3vAaezO99dZbatu2rerXr6/BgwfbHxlWvHhxh2dqT5w4Ud98840aN26sZ555xv6DQc2aNR2uxZSubKc33nhDQ4YMUb169bRp0yYdPHgw27KnTJmib7/9Vk2bNrU/4iouLk6RkZHavHmzfH19VbduXRUpUkTTpk3T+fPn5e7urhYtWigwMPC669SzZ0/17dtX77//vlq3bm2/IdeNhIWFaenSpRo9erQeeOABeXt7q0OHDvbxTzzxhMaOHasVK1bo6aeflqur683f3BsYOHCgXnnlFU2bNk2dOnVSgwYNVKJECQ0YMEDPPfecLBaLFi5cmGPwu16t/fr107Jly/SPf/xDGzZsUMOGDZWZman9+/dr2bJlioqKyvGmddKV69SLFCliD7zXevzxx/Xyyy/rs88+0+jRo/XOO++oUaNGuv/++zV06FBVqFBBR44c0ddff62dO3fecN3r1KmjAQMG6MMPP7SfVv/TTz9pwYIF6tSpk5o3by5JWrBggd5//3117txZlSpVUnJysj766CP5+PjYg/uQIUN09uxZtWjRQmXKlNHRo0f17rvvqm7dujd9bNSiRYvUpk0bderUSW3btlWrVq1UokQJnTx5UmvXrtWmTZtyHYLbt2+vSZMmKTw8XA0aNNDu3bu1ePFihzNwrpab/V1+OH78uP2ygZSUFO3du1eRkZE6efKkxowZo6eeeso+bbt27TR9+nS1adNGTzzxhE6dOqXZs2ercuXK2T7jYWFhWrt2raZPn66QkBBVqFBBDz30kNq3b6+FCxeqePHiqlGjhrZu3aq1a9dme3xhjRo11KxZM4WFhcnPz087duzQ8uXLNXz4cPs0s2fPVqNGjVS7dm09+eSTqlixouLj47V161b99ddf9meg52UfAaCAccId0wHghrIeGZb15+bmZgQHBxuPPPKIMWvWLIfH1GS59pFh69atMzp27GiEhIQYbm5uRkhIiNG7d2/j4MGDDq/78ssvjRo1ahguLi4Oj4lq2rSpUbNmzRzru94jwz799FNj3LhxRmBgoOHp6Wm0a9cu2+OIDMMw3n77baN06dKGu7u70bBhQ2PHjh3Z5nmj2q59ZJhhGEZycrIxatQoIyQkxHB1dTWqVKlivPXWWw6PPzKMK49/GjZsWLaarvcos2vFx8cb4eHhRsmSJQ03Nzejdu3aOT5a61YfGXazabMe43XtI6qyrF271mjYsKHh6elp+Pj4GB06dDD27t2bbbqNGzcaYWFhhpubm1GxYkVj7ty52fqOYVx5bNDgwYON4sWLG8WKFTN69OhhnDp1KtsjwwzDMI4ePWr079/fCAgIMNzd3Y2KFSsaw4YNM9LS0uzTfPTRR0bFihXtjyfLejRQTtvdMAwjKSnJ8PT0zPaItiw5PTIsJSXFeOKJJwxfX19DUo6P5HrssccMScYPP/yQ4/uYk+v1GcMwjAkTJjjUsWXLFuPhhx82PD09jZCQEGPs2LH2R5blttb09HRj2rRpRs2aNQ13d3ejRIkSRlhYmDFx4kTj/PnzOdaRnp5u+Pv7G40bN77hulSoUMHhUYN79uwxOnfubPj6+hoeHh5G1apVjfHjx9vHZ/WNnB49mJGRYUycONGoUKGC4erqaoSGhhrjxo1zeDzUzz//bPTu3dsoW7as4e7ubgQGBhrt27c3duzYYZ9m+fLlxqOPPmoEBgYabm5uRtmyZY2nnnrKiIuLu+G6ZLl06ZIxc+ZMo379+oaPj4/h4uJiBAcHG+3btzcWL17s8CiyrH4TGRmZbT6pqanGmDFjjFKlShmenp5Gw4YNja1bt97W/u56+9Gc9mE5KVeunP3fAYvFYvj4+Bg1a9Y0nnzySWPbtm05vmbevHlGlSpVDHd3d6NatWpGREREjp/x/fv3G02aNLF/zrL2f+fOnbPv47y9vY3WrVsb+/fvz7aPfO2114wHH3zQ8PX1NTw9PY1q1aoZr7/+upGenu6wnD/++MPo37+/ERwcbLi6uhqlS5c22rdvbyxfvtxhuuvtIwAUDhbDcOKdcwAAwB3TuXNn7d69W4cOHXJ2KSigvvvuOzVv3lyRkZHq1q2bs8sBgAKBa7oBAPgbiIuL09dff61+/fo5uxQAAP5WuKYbAIBC7PDhw9qyZYv+85//yNXV1eEaWAAAYD6OdAMAUIht3LhR/fr10+HDh7VgwQIFBwc7uyQAAP5WuKYbAAAAAACTcKQbAAAAAACTELoBAAAAADBJob+Rms1m04kTJ1SsWDFZLBZnlwMAAAAAKAQMw1BycrJCQkJktV7/eHahD90nTpxQaGios8sAAAAAABRCx44dU5kyZa47vtCH7mLFikm68kb4+Pg4uRoAAAAAQGGQlJSk0NBQe+a8nkIfurNOKffx8SF0AwAAAADy1c0uY+ZGagAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASVycXQAAAACAOy82NlYJCQnOLsM0JUuWVNmyZZ1dBkDoBgAAAP5uYmNjVbVadaVeuujsUkzj4VlUB/bvI3jD6QjdAAAAwN9MQkKCUi9dlH/7MXL1D3V2Ofku48wxnVn9thISEgjdcDpCNwAAAPA35eofKvfgys4uAyjUuJEaAAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYJK7JnS/8cYbslgsGjlypL0tNTVVw4YNk7+/v7y9vdW1a1fFx8c7r0gAAAAAAG7BXRG6t2/frg8++ED33nuvQ/uoUaO0atUqRUZGauPGjTpx4oS6dOnipCoBAAAAALg1Tg/dKSkp6tOnjz766COVKFHC3n7+/HnNmzdP06dPV4sWLRQWFqaIiAj98MMP+vHHH51YMQAAAAAAueP00D1s2DC1a9dOrVq1cmiPiYlRRkaGQ3u1atVUtmxZbd269U6XCQAAAADALXNx5sI/++wz/fzzz9q+fXu2cSdPnpSbm5t8fX0d2oOCgnTy5MnrzjMtLU1paWn24aSkJEmSzWaTzWbLn8IBAACAAswwDFmtVlktklWGs8vJd1aLZLVaZRgGGQCmyW3fclroPnbsmEaMGKHo6Gh5eHjk23ynTp2qiRMnZms/ffq0UlNT8205AAAAQEGVmpqqsLAw+QZ5yrVE4QvdGfJU6bAwpaam6tSpU84uB4VUcnJyrqZzWuiOiYnRqVOndP/999vbMjMztWnTJr333nuKiopSenq6EhMTHY52x8fHKzg4+LrzHTdunEaPHm0fTkpKUmhoqAICAuTj42PKugAAAAAFyfHjxxUTE6Pgmn3kLouzy8l3afGXdDImRh4eHgoMDHR2OSikcnvw2Gmhu2XLltq9e7dDW3h4uKpVq6YXX3xRoaGhcnV11bp169S1a1dJ0oEDBxQbG6v69etfd77u7u5yd3fP1m61WmW1Ov0SdgAAAMDpLBbLlcsvDclWCEO3zbhy6q/FYiEDwDS57VtOC93FihVTrVq1HNq8vLzk7+9vbx88eLBGjx4tPz8/+fj46Nlnn1X9+vX18MMPO6NkAAAAAABuiVNvpHYzM2bMkNVqVdeuXZWWlqbWrVvr/fffd3ZZAAAAAADkyl0Vur/77juHYQ8PD82ePVuzZ892TkEAAAAAANwGLnAAAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAEzi1NA9Z84c3XvvvfLx8ZGPj4/q16+vNWvW2MenpqZq2LBh8vf3l7e3t7p27ar4+HgnVgwAAAAAQO45NXSXKVNGb7zxhmJiYrRjxw61aNFCHTt21G+//SZJGjVqlFatWqXIyEht3LhRJ06cUJcuXZxZMgAAAAAAuebizIV36NDBYfj111/XnDlz9OOPP6pMmTKaN2+elixZohYtWkiSIiIiVL16df344496+OGHnVEyAAAAAAC55tTQfbXMzExFRkbqwoULql+/vmJiYpSRkaFWrVrZp6lWrZrKli2rrVu3Xjd0p6WlKS0tzT6clJQkSbLZbLLZbOauBAAAAFAAGIYhq9Uqq0WyynB2OfnOapGsVqsMwyADwDS57VtOD927d+9W/fr1lZqaKm9vb61YsUI1atTQzp075ebmJl9fX4fpg4KCdPLkyevOb+rUqZo4cWK29tOnTys1NTW/ywcAAAAKnNTUVIWFhck3yFOuJQpf6M6Qp0qHhSk1NVWnTp1ydjkopJKTk3M1ndNDd9WqVbVz506dP39ey5cv14ABA7Rx48Y8z2/cuHEaPXq0fTgpKUmhoaEKCAiQj49PfpQMAAAAFGjHjx9XTEyMgmv2kbsszi4n36XFX9LJmBh5eHgoMDDQ2eWgkPLw8MjVdE4P3W5ubqpcubIkKSwsTNu3b9esWbPUs2dPpaenKzEx0eFod3x8vIKDg687P3d3d7m7u2drt1qtslp5QhoAAABgsViuXH5pSLZCGLptxpVTfy0WCxkApslt37rreqDNZlNaWprCwsLk6uqqdevW2ccdOHBAsbGxql+/vhMrBAAAAAAgd5x6pHvcuHFq27atypYtq+TkZC1ZskTfffedoqKiVLx4cQ0ePFijR4+Wn5+ffHx89Oyzz6p+/frcuRwAAAAAUCA4NXSfOnVK/fv3V1xcnIoXL657771XUVFReuSRRyRJM2bMkNVqVdeuXZWWlqbWrVvr/fffd2bJAAAAAADkmlND97x582443sPDQ7Nnz9bs2bPvUEUAAAAAAOSfu+6abgAAAAAACgtCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJslT6P7zzz/zuw4AAAAAAAqdPIXuypUrq3nz5lq0aJFSU1PzuyYAAAAAAAqFPIXun3/+Wffee69Gjx6t4OBgPfXUU/rpp5/yuzYAAAAAAAq0PIXuunXratasWTpx4oQ+/vhjxcXFqVGjRqpVq5amT5+u06dP53edAAAAAAAUOLd1IzUXFxd16dJFkZGRmjZtmg4dOqTnn39eoaGh6t+/v+Li4vKrTgAAAAAACpzbCt07duzQM888o1KlSmn69Ol6/vnn9ccffyg6OlonTpxQx44d86tOAAAAAAAKHJe8vGj69OmKiIjQgQMH9Nhjj+mTTz7RY489Jqv1SoavUKGC5s+fr/Lly+dnrQAAAAAAFCh5Ct1z5szRoEGDNHDgQJUqVSrHaQIDAzVv3rzbKg4AAAAAgIIsT6H7999/v+k0bm5uGjBgQF5mDwAAAABAoZCna7ojIiIUGRmZrT0yMlILFiy47aIAAAAAACgM8hS6p06dqpIlS2ZrDwwM1JQpU267KAAAAAAACoM8he7Y2FhVqFAhW3u5cuUUGxt720UBAAAAAFAY5Cl0BwYG6tdff83WvmvXLvn7+992UQAAAAAAFAZ5Ct29e/fWc889pw0bNigzM1OZmZlav369RowYoV69euV3jQAAAAAAFEh5unv55MmTdeTIEbVs2VIuLldmYbPZ1L9/f67pBgAAAADg/+UpdLu5uWnp0qWaPHmydu3aJU9PT9WuXVvlypXL7/oAAAAAACiw8hS6s9xzzz2655578qsWAAAAAAAKlTyF7szMTM2fP1/r1q3TqVOnZLPZHMavX78+X4oDAAAAAKAgy1PoHjFihObPn6927dqpVq1aslgs+V0XAAAAAAAFXp5C92effaZly5bpsccey+96AAAAAAAoNPL0yDA3NzdVrlw5v2sBAAAAAKBQyVPoHjNmjGbNmiXDMPK7HgAAAAAACo08nV6+efNmbdiwQWvWrFHNmjXl6urqMP6LL77Il+IAAAAAACjI8hS6fX191blz5/yuBQAAAACAQiVPoTsiIiK/6wAAAAAAoNDJ0zXdknT58mWtXbtWH3zwgZKTkyVJJ06cUEpKSr4VBwAAAABAQZanI91Hjx5VmzZtFBsbq7S0ND3yyCMqVqyYpk2bprS0NM2dOze/6wQAAAAAoMDJ05HuESNGqF69ejp37pw8PT3t7Z07d9a6devyrTgAAAAAAAqyPB3p/v777/XDDz/Izc3Nob18+fI6fvx4vhQGAAAAAEBBl6cj3TabTZmZmdna//rrLxUrVuy2iwIAAAAAoDDIU+h+9NFHNXPmTPuwxWJRSkqKXn31VT322GP5VRsAAAAAAAVank4vf/vtt9W6dWvVqFFDqampeuKJJ/T777+rZMmS+vTTT/O7RgAAAAAACqQ8he4yZcpo165d+uyzz/Trr78qJSVFgwcPVp8+fRxurAYAAAAAwN9ZnkK3JLm4uKhv3775WQsAAAAAAIVKnkL3J598csPx/fv3z1MxAAAAAAAUJnkK3SNGjHAYzsjI0MWLF+Xm5qaiRYsSugEAAAAAUB7vXn7u3DmHv5SUFB04cECNGjXiRmoAAAAAAPy/PIXunFSpUkVvvPFGtqPgAAAAAAD8XeVb6Jau3FztxIkT+TlLAAAAAAAKrDxd0/3VV185DBuGobi4OL333ntq2LBhvhQGAAAAAEBBl6fQ3alTJ4dhi8WigIAAtWjRQm+//XZ+1AUAAAAAQIGXp9Bts9nyuw4AAAAAAAqdfL2mGwAAAAAA/E+ejnSPHj0619NOnz49L4sAAAAAAKDAy1Po/uWXX/TLL78oIyNDVatWlSQdPHhQRYoU0f3332+fzmKx5E+VAAAAAAAUQHkK3R06dFCxYsW0YMEClShRQpJ07tw5hYeHq3HjxhozZky+FgkAAAAAQEGUp2u63377bU2dOtUeuCWpRIkSeu2117h7OQAAAAAA/y9PoTspKUmnT5/O1n769GklJyffdlEAAAAAABQGeQrdnTt3Vnh4uL744gv99ddf+uuvv/T5559r8ODB6tKlS37XCAAAAABAgZSna7rnzp2r559/Xk888YQyMjKuzMjFRYMHD9Zbb72VrwUCAAAAAFBQ5Sl0Fy1aVO+//77eeust/fHHH5KkSpUqycvLK1+LAwAAAACgIMvT6eVZ4uLiFBcXpypVqsjLy0uGYeRXXQAAAAAAFHh5Ct1nzpxRy5Ytdc899+ixxx5TXFycJGnw4ME8LgwAAAAAgP+Xp9A9atQoubq6KjY2VkWLFrW39+zZU998802+FQcAAAAAQEGWp2u6v/32W0VFRalMmTIO7VWqVNHRo0fzpTAAAAAAAAq6PB3pvnDhgsMR7ixnz56Vu7v7bRcFAAAAAEBhkKfQ3bhxY33yySf2YYvFIpvNpjfffFPNmzfPt+IAAAAAACjI8nR6+ZtvvqmWLVtqx44dSk9P19ixY/Xbb7/p7Nmz2rJlS37XCAAAAABAgZSnI921atXSwYMH1ahRI3Xs2FEXLlxQly5d9Msvv6hSpUr5XSMAAAAAAAXSLR/pzsjIUJs2bTR37ly9/PLLZtQEAAAAAEChcMtHul1dXfXrr7+aUQsAAAAAAIVKnk4v79u3r+bNm5fftQAAAAAAUKjk6UZqly9f1scff6y1a9cqLCxMXl5eDuOnT5+eL8UBAAAAAFCQ3VLo/vPPP1W+fHnt2bNH999/vyTp4MGDDtNYLJb8qw4AAAAAgALslkJ3lSpVFBcXpw0bNkiSevbsqXfeeUdBQUGmFAcAAAAAQEF2S9d0G4bhMLxmzRpduHAhXwsCAAAAAKCwyNON1LJcG8IBAAAAAMD/3FLotlgs2a7Z5hpuAAAAAABydkvXdBuGoYEDB8rd3V2SlJqaqn/84x/Z7l7+xRdf5F+FAAAAAAAUULcUugcMGOAw3Ldv33wtBgAAAACAwuSWQndERIRZdQAAAAAAUOjc1o3UAAAAAADA9RG6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMIlTQ/fUqVP1wAMPqFixYgoMDFSnTp104MABh2lSU1M1bNgw+fv7y9vbW127dlV8fLyTKgYAAAAAIPecGro3btyoYcOG6ccff1R0dLQyMjL06KOP6sKFC/ZpRo0apVWrVikyMlIbN27UiRMn1KVLFydWDQAAAABA7rg4c+HffPONw/D8+fMVGBiomJgYNWnSROfPn9e8efO0ZMkStWjRQpIUERGh6tWr68cff9TDDz/sjLIBAAAAAMgVp4bua50/f16S5OfnJ0mKiYlRRkaGWrVqZZ+mWrVqKlu2rLZu3Zpj6E5LS1NaWpp9OCkpSZJks9lks9nMLB8AAAAoEAzDkNVqldUiWWU4u5x8Z7VIVqtVhmGQAWCa3PatuyZ022w2jRw5Ug0bNlStWrUkSSdPnpSbm5t8fX0dpg0KCtLJkydznM/UqVM1ceLEbO2nT59WampqvtcNAAAAFDSpqakKCwuTb5CnXEsUvtCdIU+VDgtTamqqTp065exyUEglJyfnarq7JnQPGzZMe/bs0ebNm29rPuPGjdPo0aPtw0lJSQoNDVVAQIB8fHxut0wAAACgwDt+/LhiYmIUXLOP3GVxdjn5Li3+kk7GxMjDw0OBgYHOLgeFlIeHR66muytC9/Dhw7V69Wpt2rRJZcqUsbcHBwcrPT1diYmJDke74+PjFRwcnOO83N3d5e7unq3darXKauUJaQAAAIDFYrly+aUh2Qph6LYZV86ktVgsZACYJrd9y6k90DAMDR8+XCtWrND69etVoUIFh/FhYWFydXXVunXr7G0HDhxQbGys6tevf6fLBQAAAADgljj1SPewYcO0ZMkSffnllypWrJj9Ou3ixYvL09NTxYsX1+DBgzV69Gj5+fnJx8dHzz77rOrXr8+dywEAAAAAdz2nhu45c+ZIkpo1a+bQHhERoYEDB0qSZsyYIavVqq5duyotLU2tW7fW+++/f4crBQAAAADg1jk1dBvGze+U6OHhodmzZ2v27Nl3oCIAAAAAAPIPdxUAAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJC7OLgAAAPx9xcbGKiEhwdllmKZkyZIqW7ass8sAADgRoRsAADhFbGysqlarrtRLF51dimk8PIvqwP59BG8A+BsjdAMAAKdISEhQ6qWL8m8/Rq7+oc4uJ99lnDmmM6vfVkJCAqEbAP7GCN0AAMCpXP1D5R5c2dllAABgCm6kBgAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASVycXQAAAABwN4qNjVVCQoKzyzDFvn37nF0C8LdB6AYAAACuERsbq6rVqiv10kVnlwKggCN0AwAAANdISEhQ6qWL8m8/Rq7+oc4uJ99d+nOHzn+/yNllAH8LhG4AAADgOlz9Q+UeXNnZZeS7jDPHnF0C8LfBjdQAAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwiVND96ZNm9ShQweFhITIYrFo5cqVDuMNw9Arr7yiUqVKydPTU61atdLvv//unGIBAAAAALhFTg3dFy5cUJ06dTR79uwcx7/55pt65513NHfuXG3btk1eXl5q3bq1UlNT73ClAAAAAADcOhdnLrxt27Zq27ZtjuMMw9DMmTP1r3/9Sx07dpQkffLJJwoKCtLKlSvVq1evO1kqAAAAAAC37K69pvvw4cM6efKkWrVqZW8rXry4HnroIW3dutWJlQEAAAAAkDtOPdJ9IydPnpQkBQUFObQHBQXZx+UkLS1NaWlp9uGkpCRJks1mk81mM6FSAADMc+zYMSUkJDi7DFPs379fVqtVVotkleHscvKd1SJZrVYZhsF3kALIMIy/Rf8s7OvH5w9mym3fumtDd15NnTpVEydOzNZ++vRprgUHABQop0+f1j+efkYZ6Wk3n7iACgsLk2+Qp1xLFL4v/RnyVOmwMKWmpurUqVPOLge3KDU1tVD3z9RQPyUX4vXj84c7ITk5OVfT3bWhOzg4WJIUHx+vUqVK2dvj4+NVt27d675u3LhxGj16tH04KSlJoaGhCggIkI+Pj2n1AgCQ344fP64ft/4g/3aj5Oof6uxy8t2lP2N0fvNiBdfsI3dZnF1OvkuLv6STMTHy8PBQYGCgs8vBLTp+/LhiYmIKbf9MOXZWZwrx+vH5w53g4eGRq+nu2tBdoUIFBQcHa926dfaQnZSUpG3btunpp5++7uvc3d3l7u6erd1qtcpqvWsvYQcAIBuLxSKbzaYifqFyDars7HLyXVrCsSuXfxmSrRB+6bcZV049tFgsfAcpgLI+f4W9fxb29ePzBzPltm85NXSnpKTo0KFD9uHDhw9r586d8vPzU9myZTVy5Ei99tprqlKliipUqKDx48crJCREnTp1cl7RAAAAAADkklND944dO9S8eXP7cNZp4QMGDND8+fM1duxYXbhwQUOHDlViYqIaNWqkb775JteH8QEAAAAAcCanhu5mzZrJMK5/4waLxaJJkyZp0qRJd7AqAAAAAADyBxc4AAAAAABgEkI3AAAAAAAmIXQDAAAAAGCSu/aRYQAAAIXBvn37nF2CaUqWLKmyZcs6uwwAuKsRugEAAEyQmXJOsljUt29fZ5diGg/Pojqwfx/BGwBugNANAABgAltaimQY8m8/Rq7+oc4uJ99lnDmmM6vfVkJCAqEbAG6A0A0AAGAiV/9QuQdXdnYZAAAn4UZqAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgElcnF0AAAAACq59+/Y5uwRTFNb1AnDnEboBAABwyzJTzkkWi/r27evsUgDgrkboBgAAwC2zpaVIhiH/9mPk6h/q7HLy3aU/d+j894ucXQaAQoDQDQAAgDxz9Q+Ve3BlZ5eR7zLOHHN2CQAKCW6kBgAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASVycXQAAwHyxsbFKSEhwdhmmKVmypMqWLevsMgAAALIhdANAIRcbG6uq1aor9dJFZ5diGg/Pojqwfx/BGwAA3HUI3QBQyCUkJCj10kX5tx8jV/9QZ5eT7zLOHNOZ1W8rISGB0A0AAO46hG4A+Jtw9Q+Ve3BlZ5cBAADwt8KN1AAAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADCJi7MLwP/ExsYqISHB2WWYpmTJkipbtqyzyzAN269gK8zbb9++fc4u4Y4ojOtZGNcJAJA/CvN3F6lwffckdN8lYmNjVbVadaVeuujsUkzj4VlUB/bvKzQfnqux/Qq2v8P2K8wyU85JFov69u3r7FIAALgj/g7fXQrTd09C910iISFBqZcuyr/9GLn6hzq7nHyXceaYzqx+WwkJCYXig3Mttl/BVti336U/d+j894ucXYZpbGkpkmEUyu1X2LcdACBvCvt3l8L23ZPQfZdx9Q+Ve3BlZ5eBPGL7FWyFdftlnDnm7BLuiMK4/f4u2w4AkDeF8d++wogbqQEAAAAAYBJCNwAAAAAAJikQoXv27NkqX768PDw89NBDD+mnn35ydkkAAAAAANzUXR+6ly5dqtGjR+vVV1/Vzz//rDp16qh169Y6deqUs0sDAAAAAOCG7vrQPX36dD355JMKDw9XjRo1NHfuXBUtWlQff/yxs0sDAAAAAOCG7urQnZ6erpiYGLVq1creZrVa1apVK23dutWJlQEAAAAAcHN39SPDEhISlJmZqaCgIIf2oKAg7d+/P8fXpKWlKS0tzT58/vx5SVJiYqJsNpt5xd6m5ORkWSwWZcQfkjJSnV1Ovss4+5csFotiYmKUnJzs7HLy3cGDB9l+BVhh336Z/7/9WL+CpzCvm8T6FXSsX8FW2NeP7y4FW9b2S05OVmJiorPLua6kpCRJkmEYN5zOYtxsCic6ceKESpcurR9++EH169e3t48dO1YbN27Utm3bsr1mwoQJmjhx4p0sEwAAAADwN3Xs2DGVKVPmuuPv6iPdJUuWVJEiRRQfH+/QHh8fr+Dg4BxfM27cOI0ePdo+bLPZdPbsWfn7+8tisZhaL+4uSUlJCg0N1bFjx+Tj4+PscoB8Qb9GYUXfRmFF30ZhRL++wjAMJScnKyQk5IbT3dWh283NTWFhYVq3bp06deok6UqIXrdunYYPH57ja9zd3eXu7u7Q5uvra3KluJv5+Pj8rXcGKJzo1yis6NsorOjbKIzo11Lx4sVvOs1dHbolafTo0RowYIDq1aunBx98UDNnztSFCxcUHh7u7NIAAAAAALihuz509+zZU6dPn9Yrr7yikydPqm7duvrmm2+y3VwNAAAAAIC7zV0fuiVp+PDh1z2dHLged3d3vfrqq9kuNwAKMvo1Civ6Ngor+jYKI/r1rbmr714OAAAAAEBBZnV2AQAAAAAAFFaEbgAAAAAATELoBgAAAADAJIRuFHjHjx9X37595e/vL09PT9WuXVs7duywjzcMQ6+88opKlSolT09PtWrVSr///rsTKwZurnz58rJYLNn+hg0bJklKTU3VsGHD5O/vL29vb3Xt2lXx8fFOrhq4sczMTI0fP14VKlSQp6enKlWqpMmTJ+vq28uwz0ZBlZycrJEjR6pcuXLy9PRUgwYNtH37dvt4+jYKgk2bNqlDhw4KCQmRxWLRypUrHcbnph+fPXtWffr0kY+Pj3x9fTV48GClpKTcwbW4+xC6UaCdO3dODRs2lKurq9asWaO9e/fq7bffVokSJezTvPnmm3rnnXc0d+5cbdu2TV5eXmrdurVSU1OdWDlwY9u3b1dcXJz9Lzo6WpLUvXt3SdKoUaO0atUqRUZGauPGjTpx4oS6dOnizJKBm5o2bZrmzJmj9957T/v27dO0adP05ptv6t1337VPwz4bBdWQIUMUHR2thQsXavfu3Xr00UfVqlUrHT9+XBJ9GwXDhQsXVKdOHc2ePTvH8bnpx3369NFvv/2m6OhorV69Wps2bdLQoUPv1CrcnQygAHvxxReNRo0aXXe8zWYzgoODjbfeesvelpiYaLi7uxuffvrpnSgRyBcjRowwKlWqZNhsNiMxMdFwdXU1IiMj7eP37dtnSDK2bt3qxCqBG2vXrp0xaNAgh7YuXboYffr0MQyDfTYKrosXLxpFihQxVq9e7dB+//33Gy+//DJ9GwWSJGPFihX24dz047179xqSjO3bt9unWbNmjWGxWIzjx4/fsdrvNhzpRoH21VdfqV69eurevbsCAwN133336aOPPrKPP3z4sE6ePKlWrVrZ24oXL66HHnpIW7dudUbJwC1LT0/XokWLNGjQIFksFsXExCgjI8OhX1erVk1ly5alX+Ou1qBBA61bt04HDx6UJO3atUubN29W27ZtJbHPRsF1+fJlZWZmysPDw6Hd09NTmzdvpm+jUMhNP966dat8fX1Vr149+zStWrWS1WrVtm3b7njNdwtCNwq0P//8U3PmzFGVKlUUFRWlp59+Ws8995wWLFggSTp58qQkKSgoyOF1QUFB9nHA3W7lypVKTEzUwIEDJV3p125ubvL19XWYjn6Nu90///lP9erVS9WqVZOrq6vuu+8+jRw5Un369JHEPhsFV7FixVS/fn1NnjxZJ06cUGZmphYtWqStW7cqLi6Ovo1CITf9+OTJkwoMDHQY7+LiIj8/v791X3dxdgHA7bDZbKpXr56mTJkiSbrvvvu0Z88ezZ07VwMGDHBydUD+mDdvntq2bauQkBBnlwLclmXLlmnx4sVasmSJatasqZ07d2rkyJEKCQlhn40Cb+HChRo0aJBKly6tIkWK6P7771fv3r0VExPj7NIAOBlHulGglSpVSjVq1HBoq169umJjYyVJwcHBkpTtrs7x8fH2ccDd7OjRo1q7dq2GDBlibwsODlZ6eroSExMdpqVf4273wgsv2I92165dW/369dOoUaM0depUSeyzUbBVqlRJGzduVEpKio4dO6affvpJGRkZqlixIn0bhUJu+nFwcLBOnTrlMP7y5cs6e/bs37qvE7pRoDVs2FAHDhxwaDt48KDKlSsnSapQoYKCg4O1bt06+/ikpCRt27ZN9evXv6O1AnkRERGhwMBAtWvXzt4WFhYmV1dXh3594MABxcbG0q9xV7t48aKsVsevHkWKFJHNZpPEPhuFg5eXl0qVKqVz584pKipKHTt2pG+jUMhNP65fv74SExMdzvBYv369bDabHnrooTte892C08tRoI0aNUoNGjTQlClT1KNHD/3000/68MMP9eGHH0qSLBaLRo4cqddee01VqlRRhQoVNH78eIWEhKhTp07OLR64CZvNpoiICA0YMEAuLv/bXRcvXlyDBw/W6NGj5efnJx8fHz377LOqX7++Hn74YSdWDNxYhw4d9Prrr6ts2bKqWbOmfvnlF02fPl2DBg2SxD4bBVtUVJQMw1DVqlV16NAhvfDCC6pWrZrCw8Pp2ygwUlJSdOjQIfvw4cOHtXPnTvn5+als2bI37cfVq1dXmzZt9OSTT2ru3LnKyMjQ8OHD1atXr7/3ZXLOvn06cLtWrVpl1KpVy3B3dzeqVatmfPjhhw7jbTabMX78eCMoKMhwd3c3WrZsaRw4cMBJ1QK5FxUVZUjKsb9eunTJeOaZZ4wSJUoYRYsWNTp37mzExcU5oUog95KSkowRI0YYZcuWNTw8PIyKFSsaL7/8spGWlmafhn02CqqlS5caFStWNNzc3Izg4GBj2LBhRmJion08fRsFwYYNGwxJ2f4GDBhgGEbu+vGZM2eM3r17G97e3oaPj48RHh5uJCcnO2Ft7h4WwzAMZ4Z+AAAAAAAKK67pBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAOAuM3DgQHXq1Mn05ZQvX14zZ87M9fTNmjXTyJEjTasHAIDCiNANAEAuDBw4UBaLRRaLRW5ubqpcubImTZqky5cvO7u0m5o/f758fX2ztW/fvl1Dhw7N9Xy++OILTZ482T58q6H9eq5+b11dXVWhQgWNHTtWqamptzQffhQAANyNXJxdAAAABUWbNm0UERGhtLQ0/fe//9WwYcPk6uqqcePGZZs2PT1dbm5uTqgy9wICAm5pej8/P5Mq+d97m5GRoZiYGA0YMEAWi0XTpk0zbZkAANwJHOkGACCX3N3dFRwcrHLlyunpp59Wq1at9NVXX0n63ynhr7/+ukJCQlS1alVJ0u7du9WiRQt5enrK399fQ4cOVUpKin2emZmZGj16tHx9feXv76+xY8fKMAyH5eZ0RLlu3bqaMGGCfTgxMVFPPfWUgoKC5OHhoVq1amn16tX67rvvFB4ervPnz9uPJme97ur5PvHEE+rZs6fDMjIyMlSyZEl98sknkhyPJDdr1kxHjx7VqFGj7PO9cOGCfHx8tHz5cof5rFy5Ul5eXkpOTr7pexsaGqpOnTqpVatWio6Oto8/c+aMevfurdKlS6to0aKqXbu2Pv30U/v4gQMHauPGjZo1a5a9niNHjkiS9uzZo7Zt28rb21tBQUHq16+fEhISrlsLAAD5idANAEAeeXp6Kj093T68bt06HThwQNHR0Vq9erUuXLig1q1bq0SJEtq+fbsiIyO1du1aDR8+3P6at99+W/Pnz9fHH3+szZs36+zZs1qxYsUt1WGz2dS2bVtt2bJFixYt0t69e/XGG2+oSJEiatCggWbOnCkfHx/FxcUpLi5Ozz//fLZ59OnTR6tWrXL4QSAqKkoXL15U586ds03/xRdfqEyZMpo0aZJ9vl5eXurVq5ciIiIcpo2IiFC3bt1UrFixXK3Pnj179MMPPzicKZCamqqwsDB9/fXX2rNnj4YOHap+/frpp59+kiTNmjVL9evX15NPPmmvJzQ0VImJiWrRooXuu+8+7dixQ998843i4+PVo0ePXNUCAMDt4vRyAABukWEYWrdunaKiovTss8/a2728vPSf//zHHhY/+ugjpaam6pNPPpGXl5ck6b333lOHDh00bdo0BQUFaebMmRo3bpy6dOkiSZo7d66ioqJuqZ61a9fqp59+0r59+3TPPfdIkipWrGgfX7x4cVksFgUHB193Hq1bt5aXl5dWrFihfv36SZKWLFmixx9/PMew7OfnpyJFiqhYsWIO8x0yZIgaNGiguLg4lSpVSqdOndJ///tfrV279obrsHr1anl7e+vy5ctKS0uT1WrVe++9Zx9funRphx8Lnn32WUVFRWnZsmV68MEHVbx4cbm5ualo0aIO9bz33nu67777NGXKFHvbxx9/rNDQUB08eND+fgEAYBaOdAMAkEtZwdDDw0Nt27ZVz549HU7xrl27tsPR2X379qlOnTr2wC1JDRs2lM1m04EDB3T+/HnFxcXpoYceso93cXFRvXr1bqmunTt3qkyZMrcVIF1cXNSjRw8tXrxYknThwgV9+eWX6tOnzy3N58EHH1TNmjW1YMECSdKiRYtUrlw5NWnS5Iava968uXbu3Klt27ZpwIABCg8PV9euXe3jMzMzNXnyZNWuXVt+fn7y9vZWVFSUYmNjbzjfXbt2acOGDfL29rb/VatWTZL0xx9/3NK6AQCQFxzpBgAgl5o3b645c+bIzc1NISEhcnFx/Gf06nCdn6xWa7brvDMyMuz/7+npmS/L6dOnj5o2bapTp04pOjpanp6eatOmzS3PZ8iQIZo9e7b++c9/KiIiQuHh4bJYLDd8jZeXlypXrizpypHoOnXqaN68eRo8eLAk6a233tKsWbM0c+ZM1a5dW15eXho5cqTD6f05SUlJsZ9ZcK1SpUrd8roBAHCrONINAEAuZQXDsmXLZgvcOalevbp27dqlCxcu2Nu2bNkiq9WqqlWrqnjx4ipVqpS2bdtmH3/58mXFxMQ4zCcgIEBxcXH24aSkJB0+fNg+fO+99+qvv/7SwYMHc6zDzc1NmZmZN623QYMGCg0N1dKlS7V48WJ1795drq6u153+evPt27evjh49qnfeeUd79+7VgAEDbrrsq1mtVr300kv617/+pUuXLkm68r517NhRffv2VZ06dVSxYsVs65tTPffff79+++03lS9fXpUrV3b4M+tHEgAArkboBgDAJH369JGHh4cGDBigPXv2aMOGDXr22WfVr18/BQUFSZJGjBihN954QytXrtT+/fv1zDPPKDEx0WE+LVq00MKFC/X9999r9+7dGjBggIoUKWIf37RpUzVp0kRdu3ZVdHS0Dh8+rDVr1uibb76RdOUu5SkpKVq3bp0SEhJ08eLF69b8xBNPaO7cuYqOjr7pqeXly5fXpk2bdPz4cYe7gZcoUUJdunTRCy+8oEcffVRlypS51bdO3bt3V5EiRTR79mxJUpUqVRQdHa0ffvhB+/bt01NPPaX4+Phs9Wzbtk1HjhxRQkKCbDabhg0bprNnz6p3797avn27/vjjD0VFRSk8PDxXP0QAAHC7CN0AAJikaNGiioqK0tmzZ/XAAw+oW7duatmypcMNwsaMGaN+/fppwIABql+/vooVK5btbuHjxo1T06ZN1b59e7Vr106dOnVSpUqVHKb5/PPP9cADD6h3796qUaOGxo4daw+VDRo00D/+8Q/17NlTAQEBevPNN69bc58+fbR3716VLl1aDRs2vOH6TZo0SUeOHFGlSpWyPfN78ODBSk9P16BBg3L1Xl3LxcVFw4cP15tvvqkLFy7oX//6l+6//361bt1azZo1U3BwsDp16uTwmueff15FihRRjRo1FBAQoNjYWIWEhGjLli3KzMzUo48+qtq1a2vkyJHy9fWV1crXIACA+SzGtReJAQAA3KaFCxdq1KhROnHihMPN5QAA+LvhRmoAACDfXLx4UXFxcXrjjTf01FNPEbgBAH97nFcFAADyzZtvvqlq1aopODhY48aNc3Y5AAA4HaeXAwAAAABgEo50AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGCS/wO17DkmfzmgCQAAAABJRU5ErkJggg==\n"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Productivity rate = (Eggs Laid / Number of Hens) x 100.\n\nThe majority of days are over 90, which is a high productivity rate. The smallest rate was under 60 one day and the largest was a perfect 100.","metadata":{}},{"cell_type":"code","source":"# padding so model can expect a fixed set of dimensions from graphs\n\nmax_node_dim = max(g.x.shape[1] for g in graphs)\nmax_edge_dim = max(g.edge_attr.shape[1] for g in graphs)\n\ndef pad_features(graph, max_node_dim, max_edge_dim):\n    # Pad node features\n    node_feat = graph.x\n    if node_feat.shape[1] < max_node_dim:\n        pad_size = max_node_dim - node_feat.shape[1]\n        padding = torch.zeros((node_feat.shape[0], pad_size), dtype=node_feat.dtype)\n        graph.x = torch.cat([node_feat, padding], dim=1)\n    \n    # Pad edge features\n    edge_feat = graph.edge_attr\n    if edge_feat.shape[1] < max_edge_dim:\n        pad_size = max_edge_dim - edge_feat.shape[1]\n        padding = torch.zeros((edge_feat.shape[0], pad_size), dtype=edge_feat.dtype)\n        graph.edge_attr = torch.cat([edge_feat, padding], dim=1)\n    \n    return graph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:32:53.239082Z","iopub.execute_input":"2025-07-02T09:32:53.239504Z","iopub.status.idle":"2025-07-02T09:32:53.245609Z","shell.execute_reply.started":"2025-07-02T09:32:53.239484Z","shell.execute_reply":"2025-07-02T09:32:53.244809Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"graphs_padded = [pad_features(g, max_node_dim, max_edge_dim) for g in graphs]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T09:35:39.321852Z","iopub.execute_input":"2025-07-02T09:35:39.322551Z","iopub.status.idle":"2025-07-02T09:35:39.407609Z","shell.execute_reply.started":"2025-07-02T09:35:39.322526Z","shell.execute_reply":"2025-07-02T09:35:39.406997Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### 6) Create GAT Model for Graph Regression¶\n\nGAT model takes graph as input, applies GAT layers to learn node embeddings, aggregates (mean pooling or attention pooling) to a graph level embedding, and passes that to fully connected layers to predict a single value (productivity)","metadata":{}},{"cell_type":"markdown","source":"global mean pooling","metadata":{}},{"cell_type":"code","source":"# using edge_attr properly\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATv2Conv, global_mean_pool\nfrom torch_geometric.loader import DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport numpy as np\n\nclass GATGraphRegressor(torch.nn.Module):\n    def __init__(self, in_node_feats, in_edge_feats, hidden_dim=64, heads=4):\n        super(GATGraphRegressor, self).__init__()\n        self.gat1 = GATv2Conv(in_node_feats, hidden_dim, heads=heads, concat=True, dropout=0.2, edge_dim=in_edge_feats)\n        self.gat2 = GATv2Conv(hidden_dim * heads, hidden_dim, heads=1, concat=True, dropout=0.2, edge_dim=in_edge_feats)\n\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        x = self.gat1(x, edge_index, edge_attr)\n        x = F.elu(x)\n        x = self.gat2(x, edge_index, edge_attr)\n        x = F.elu(x)\n        x = global_mean_pool(x, batch)\n        return self.mlp(x).squeeze(1)\n\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    losses = []\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y.view(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        losses.append(loss.item())\n    return np.mean(losses)\n\ndef eval_model(model, loader, criterion, device):\n    model.eval()\n    losses = []\n    preds, targets = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            losses.append(loss.item())\n            preds.extend(out.cpu().numpy())\n            targets.extend(batch.y.view(-1).cpu().numpy())\n    rmse = mean_squared_error(targets, preds, squared=False)\n    mae = mean_absolute_error(targets, preds)\n    return np.mean(losses), rmse, mae\n\n# Replace these with your actual data and dimensions\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\n# Make sure `graphs_padded` is defined and has x, edge_index, edge_attr, y, batch\n# Also make sure all graphs have same node feature dimension and edge feature dimension\nin_node_feats = graphs_padded[0].x.shape[1]\nin_edge_feats = graphs_padded[0].edge_attr.shape[1]\n\nall_rmse = []\nall_mae = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(graphs_padded)):\n    print(f\"Fold {fold + 1}\")\n    train_dataset = [graphs_padded[i] for i in train_idx]\n    val_dataset = [graphs_padded[i] for i in val_idx]\n\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    model = GATGraphRegressor(in_node_feats=in_node_feats, in_edge_feats=in_edge_feats).to(device)\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n    best_val_loss = float('inf')\n    patience = 10\n    patience_counter = 0\n\n    for epoch in range(100):\n        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, rmse, mae = eval_model(model, val_loader, criterion, device)\n        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), f\"best_model_fold{fold}.pt\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping\")\n                break\n\n    # Load best model and evaluate on validation set\n    model.load_state_dict(torch.load(f\"best_model_fold{fold}.pt\"))\n    _, rmse, mae = eval_model(model, val_loader, criterion, device)\n    all_rmse.append(rmse)\n    all_mae.append(mae)\n    print(f\"Fold {fold + 1} final RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\nprint(f\"\\nAverage RMSE: {np.mean(all_rmse):.4f} ± {np.std(all_rmse):.4f}\")\nprint(f\"Average MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:52:09.650227Z","iopub.execute_input":"2025-07-02T10:52:09.650913Z","iopub.status.idle":"2025-07-02T11:09:19.899245Z","shell.execute_reply.started":"2025-07-02T10:52:09.650890Z","shell.execute_reply":"2025-07-02T11:09:19.898511Z"}},"outputs":[{"name":"stdout","text":"Fold 1\nEpoch 0 | Train Loss: 1.1058 | Val Loss: 0.6407 | RMSE: 0.8005 | MAE: 0.7449\nEpoch 1 | Train Loss: 1.1123 | Val Loss: 0.6330 | RMSE: 0.7956 | MAE: 0.7388\nEpoch 2 | Train Loss: 1.1049 | Val Loss: 0.6322 | RMSE: 0.7951 | MAE: 0.7386\nEpoch 3 | Train Loss: 1.1067 | Val Loss: 0.6275 | RMSE: 0.7921 | MAE: 0.7344\nEpoch 4 | Train Loss: 1.1046 | Val Loss: 0.6226 | RMSE: 0.7890 | MAE: 0.7308\nEpoch 5 | Train Loss: 1.0996 | Val Loss: 0.6229 | RMSE: 0.7893 | MAE: 0.7321\nEpoch 6 | Train Loss: 1.0867 | Val Loss: 0.6180 | RMSE: 0.7861 | MAE: 0.7291\nEpoch 7 | Train Loss: 1.0693 | Val Loss: 0.6173 | RMSE: 0.7857 | MAE: 0.7297\nEpoch 8 | Train Loss: 1.0535 | Val Loss: 0.6114 | RMSE: 0.7819 | MAE: 0.7268\nEpoch 9 | Train Loss: 1.0162 | Val Loss: 0.5892 | RMSE: 0.7676 | MAE: 0.7104\nEpoch 10 | Train Loss: 0.9994 | Val Loss: 0.5649 | RMSE: 0.7516 | MAE: 0.6953\nEpoch 11 | Train Loss: 0.9208 | Val Loss: 0.5615 | RMSE: 0.7493 | MAE: 0.6897\nEpoch 12 | Train Loss: 0.8515 | Val Loss: 0.5563 | RMSE: 0.7459 | MAE: 0.6881\nEpoch 13 | Train Loss: 0.7505 | Val Loss: 0.5328 | RMSE: 0.7299 | MAE: 0.6718\nEpoch 14 | Train Loss: 0.6282 | Val Loss: 0.4769 | RMSE: 0.6905 | MAE: 0.6216\nEpoch 15 | Train Loss: 0.5300 | Val Loss: 0.4162 | RMSE: 0.6451 | MAE: 0.5612\nEpoch 16 | Train Loss: 0.4304 | Val Loss: 0.4248 | RMSE: 0.6518 | MAE: 0.5596\nEpoch 17 | Train Loss: 0.3721 | Val Loss: 0.4289 | RMSE: 0.6549 | MAE: 0.5680\nEpoch 18 | Train Loss: 0.3272 | Val Loss: 0.4642 | RMSE: 0.6813 | MAE: 0.5834\nEpoch 19 | Train Loss: 0.2976 | Val Loss: 0.3864 | RMSE: 0.6216 | MAE: 0.5302\nEpoch 20 | Train Loss: 0.2744 | Val Loss: 0.4481 | RMSE: 0.6694 | MAE: 0.5510\nEpoch 21 | Train Loss: 0.2535 | Val Loss: 0.4517 | RMSE: 0.6721 | MAE: 0.5755\nEpoch 22 | Train Loss: 0.2489 | Val Loss: 0.3911 | RMSE: 0.6254 | MAE: 0.5057\nEpoch 23 | Train Loss: 0.2400 | Val Loss: 0.4455 | RMSE: 0.6675 | MAE: 0.5312\nEpoch 24 | Train Loss: 0.2383 | Val Loss: 0.4290 | RMSE: 0.6550 | MAE: 0.5517\nEpoch 25 | Train Loss: 0.2387 | Val Loss: 0.4292 | RMSE: 0.6551 | MAE: 0.5456\nEpoch 26 | Train Loss: 0.2045 | Val Loss: 0.4235 | RMSE: 0.6507 | MAE: 0.5512\nEpoch 27 | Train Loss: 0.2058 | Val Loss: 0.4575 | RMSE: 0.6764 | MAE: 0.5607\nEpoch 28 | Train Loss: 0.1942 | Val Loss: 0.3881 | RMSE: 0.6230 | MAE: 0.4976\nEpoch 29 | Train Loss: 0.1816 | Val Loss: 0.4252 | RMSE: 0.6520 | MAE: 0.5214\nEarly stopping\nFold 1 final RMSE: 0.6216, MAE: 0.5302\nFold 2\nEpoch 0 | Train Loss: 1.1239 | Val Loss: 0.5022 | RMSE: 0.7086 | MAE: 0.6739\nEpoch 1 | Train Loss: 1.1304 | Val Loss: 0.4916 | RMSE: 0.7012 | MAE: 0.6668\nEpoch 2 | Train Loss: 1.1273 | Val Loss: 0.4857 | RMSE: 0.6970 | MAE: 0.6626\nEpoch 3 | Train Loss: 1.1313 | Val Loss: 0.4730 | RMSE: 0.6877 | MAE: 0.6528\nEpoch 4 | Train Loss: 1.1393 | Val Loss: 0.4698 | RMSE: 0.6854 | MAE: 0.6502\nEpoch 5 | Train Loss: 1.1307 | Val Loss: 0.4547 | RMSE: 0.6743 | MAE: 0.6377\nEpoch 6 | Train Loss: 1.1232 | Val Loss: 0.4454 | RMSE: 0.6674 | MAE: 0.6296\nEpoch 7 | Train Loss: 1.1141 | Val Loss: 0.4353 | RMSE: 0.6598 | MAE: 0.6214\nEpoch 8 | Train Loss: 1.1015 | Val Loss: 0.4276 | RMSE: 0.6539 | MAE: 0.6155\nEpoch 9 | Train Loss: 1.0505 | Val Loss: 0.4251 | RMSE: 0.6520 | MAE: 0.6152\nEpoch 10 | Train Loss: 1.0087 | Val Loss: 0.4000 | RMSE: 0.6324 | MAE: 0.5936\nEpoch 11 | Train Loss: 0.9460 | Val Loss: 0.3805 | RMSE: 0.6168 | MAE: 0.5830\nEpoch 12 | Train Loss: 0.8497 | Val Loss: 0.3405 | RMSE: 0.5835 | MAE: 0.5446\nEpoch 13 | Train Loss: 0.7883 | Val Loss: 0.3322 | RMSE: 0.5763 | MAE: 0.5465\nEpoch 14 | Train Loss: 0.6637 | Val Loss: 0.2795 | RMSE: 0.5287 | MAE: 0.4891\nEpoch 15 | Train Loss: 0.5980 | Val Loss: 0.2915 | RMSE: 0.5399 | MAE: 0.4975\nEpoch 16 | Train Loss: 0.4560 | Val Loss: 0.2288 | RMSE: 0.4783 | MAE: 0.4263\nEpoch 17 | Train Loss: 0.3875 | Val Loss: 0.2972 | RMSE: 0.5452 | MAE: 0.4771\nEpoch 18 | Train Loss: 0.3265 | Val Loss: 0.2601 | RMSE: 0.5100 | MAE: 0.4301\nEpoch 19 | Train Loss: 0.2851 | Val Loss: 0.2112 | RMSE: 0.4596 | MAE: 0.3750\nEpoch 20 | Train Loss: 0.2680 | Val Loss: 0.2503 | RMSE: 0.5003 | MAE: 0.4156\nEpoch 21 | Train Loss: 0.2420 | Val Loss: 0.2644 | RMSE: 0.5142 | MAE: 0.4325\nEpoch 22 | Train Loss: 0.2290 | Val Loss: 0.2020 | RMSE: 0.4495 | MAE: 0.3593\nEpoch 23 | Train Loss: 0.1994 | Val Loss: 0.2640 | RMSE: 0.5138 | MAE: 0.4328\nEpoch 24 | Train Loss: 0.1826 | Val Loss: 0.2241 | RMSE: 0.4734 | MAE: 0.3682\nEpoch 25 | Train Loss: 0.1722 | Val Loss: 0.2168 | RMSE: 0.4656 | MAE: 0.3494\nEpoch 26 | Train Loss: 0.1618 | Val Loss: 0.2400 | RMSE: 0.4899 | MAE: 0.3837\nEpoch 27 | Train Loss: 0.1631 | Val Loss: 0.2311 | RMSE: 0.4807 | MAE: 0.3803\nEpoch 28 | Train Loss: 0.1406 | Val Loss: 0.1905 | RMSE: 0.4364 | MAE: 0.3328\nEpoch 29 | Train Loss: 0.1534 | Val Loss: 0.2472 | RMSE: 0.4972 | MAE: 0.4021\nEpoch 30 | Train Loss: 0.1331 | Val Loss: 0.2584 | RMSE: 0.5083 | MAE: 0.4132\nEpoch 31 | Train Loss: 0.1263 | Val Loss: 0.1989 | RMSE: 0.4460 | MAE: 0.3354\nEpoch 32 | Train Loss: 0.1263 | Val Loss: 0.2901 | RMSE: 0.5386 | MAE: 0.4382\nEpoch 33 | Train Loss: 0.1166 | Val Loss: 0.2585 | RMSE: 0.5084 | MAE: 0.4011\nEpoch 34 | Train Loss: 0.1064 | Val Loss: 0.3101 | RMSE: 0.5569 | MAE: 0.4675\nEpoch 35 | Train Loss: 0.1027 | Val Loss: 0.2879 | RMSE: 0.5366 | MAE: 0.4407\nEpoch 36 | Train Loss: 0.0954 | Val Loss: 0.2782 | RMSE: 0.5275 | MAE: 0.4159\nEpoch 37 | Train Loss: 0.0880 | Val Loss: 0.2706 | RMSE: 0.5202 | MAE: 0.4093\nEpoch 38 | Train Loss: 0.0937 | Val Loss: 0.2923 | RMSE: 0.5407 | MAE: 0.4490\nEarly stopping\nFold 2 final RMSE: 0.4364, MAE: 0.3327\nFold 3\nEpoch 0 | Train Loss: 0.8633 | Val Loss: 1.5739 | RMSE: 1.2546 | MAE: 0.9297\nEpoch 1 | Train Loss: 0.8560 | Val Loss: 1.5756 | RMSE: 1.2552 | MAE: 0.9257\nEpoch 2 | Train Loss: 0.8564 | Val Loss: 1.5776 | RMSE: 1.2560 | MAE: 0.9223\nEpoch 3 | Train Loss: 0.8477 | Val Loss: 1.5803 | RMSE: 1.2571 | MAE: 0.9197\nEpoch 4 | Train Loss: 0.8470 | Val Loss: 1.6018 | RMSE: 1.2656 | MAE: 0.9103\nEpoch 5 | Train Loss: 0.8507 | Val Loss: 1.6168 | RMSE: 1.2715 | MAE: 0.9038\nEpoch 6 | Train Loss: 0.8595 | Val Loss: 1.6369 | RMSE: 1.2794 | MAE: 0.8974\nEpoch 7 | Train Loss: 0.8804 | Val Loss: 1.6453 | RMSE: 1.2827 | MAE: 0.8905\nEpoch 8 | Train Loss: 0.8547 | Val Loss: 1.6399 | RMSE: 1.2806 | MAE: 0.8864\nEpoch 9 | Train Loss: 0.8505 | Val Loss: 1.6520 | RMSE: 1.2853 | MAE: 0.8776\nEpoch 10 | Train Loss: 0.8590 | Val Loss: 1.6600 | RMSE: 1.2884 | MAE: 0.8688\nEarly stopping\nFold 3 final RMSE: 1.2546, MAE: 0.9297\nFold 4\nEpoch 0 | Train Loss: 0.9986 | Val Loss: 1.1362 | RMSE: 1.0659 | MAE: 0.9032\nEpoch 1 | Train Loss: 0.9983 | Val Loss: 1.1547 | RMSE: 1.0746 | MAE: 0.9027\nEpoch 2 | Train Loss: 1.0107 | Val Loss: 1.1849 | RMSE: 1.0885 | MAE: 0.9053\nEpoch 3 | Train Loss: 1.0026 | Val Loss: 1.2072 | RMSE: 1.0987 | MAE: 0.9069\nEpoch 4 | Train Loss: 1.0188 | Val Loss: 1.2223 | RMSE: 1.1056 | MAE: 0.9061\nEpoch 5 | Train Loss: 1.0255 | Val Loss: 1.2485 | RMSE: 1.1174 | MAE: 0.9075\nEpoch 6 | Train Loss: 1.0384 | Val Loss: 1.2655 | RMSE: 1.1249 | MAE: 0.9061\nEpoch 7 | Train Loss: 1.0337 | Val Loss: 1.2973 | RMSE: 1.1390 | MAE: 0.9073\nEpoch 8 | Train Loss: 1.0167 | Val Loss: 1.2739 | RMSE: 1.1287 | MAE: 0.9009\nEpoch 9 | Train Loss: 1.0134 | Val Loss: 1.2881 | RMSE: 1.1349 | MAE: 0.8994\nEpoch 10 | Train Loss: 0.9957 | Val Loss: 1.2953 | RMSE: 1.1381 | MAE: 0.8960\nEarly stopping\nFold 4 final RMSE: 1.0659, MAE: 0.9032\nFold 5\nEpoch 0 | Train Loss: 0.9695 | Val Loss: 1.1538 | RMSE: 1.0741 | MAE: 0.9005\nEpoch 1 | Train Loss: 0.9578 | Val Loss: 1.2207 | RMSE: 1.1049 | MAE: 0.9172\nEpoch 2 | Train Loss: 0.9567 | Val Loss: 1.2956 | RMSE: 1.1382 | MAE: 0.9385\nEpoch 3 | Train Loss: 0.9506 | Val Loss: 1.3914 | RMSE: 1.1796 | MAE: 0.9645\nEpoch 4 | Train Loss: 0.9478 | Val Loss: 1.4873 | RMSE: 1.2196 | MAE: 0.9875\nEpoch 5 | Train Loss: 0.9832 | Val Loss: 1.6096 | RMSE: 1.2687 | MAE: 1.0160\nEpoch 6 | Train Loss: 1.0030 | Val Loss: 1.7131 | RMSE: 1.3089 | MAE: 1.0417\nEpoch 7 | Train Loss: 1.0009 | Val Loss: 1.7828 | RMSE: 1.3352 | MAE: 1.0606\nEpoch 8 | Train Loss: 1.0630 | Val Loss: 1.9213 | RMSE: 1.3861 | MAE: 1.0987\nEpoch 9 | Train Loss: 1.0468 | Val Loss: 1.8833 | RMSE: 1.3723 | MAE: 1.0880\nEpoch 10 | Train Loss: 1.0531 | Val Loss: 1.9085 | RMSE: 1.3815 | MAE: 1.0958\nEarly stopping\nFold 5 final RMSE: 1.0741, MAE: 0.9005\n\nAverage RMSE: 0.8905 ± 0.3084\nAverage MAE: 0.7193 ± 0.2433\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"global_max_pool instead of global_mean_pool","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv, global_max_pool\nfrom torch_geometric.loader import DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport numpy as np\n\nclass GATGraphRegressor(torch.nn.Module):\n    def __init__(self, in_node_feats, edge_dim, hidden_dim=64, heads=4):\n        super(GATGraphRegressor, self).__init__()\n        self.gat1 = GATConv(in_node_feats, hidden_dim, heads=heads, concat=True, dropout=0.2, edge_dim=edge_dim, add_self_loops=False)\n        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=True, dropout=0.2, edge_dim=edge_dim, add_self_loops=False)\n\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        x = self.gat1(x, edge_index, edge_attr)\n        x = F.elu(x)\n        x = self.gat2(x, edge_index, edge_attr)\n        x = F.elu(x)\n        x = global_max_pool(x, batch)\n        return self.mlp(x).squeeze(1)\n\n# Normalize targets\nys_all = [g.y.item() for g in graphs_padded]\nmean_y = np.mean(ys_all)\nstd_y = np.std(ys_all)\n\nfor g in graphs_padded:\n    g.y = torch.tensor([(g.y.item() - mean_y) / std_y], dtype=torch.float)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    losses = []\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y.view(-1))\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    return np.mean(losses)\n\ndef eval_model(model, loader, criterion):\n    model.eval()\n    losses = []\n    preds, targets = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            losses.append(loss.item())\n            preds.extend(out.cpu().numpy())\n            targets.extend(batch.y.view(-1).cpu().numpy())\n    rmse = mean_squared_error(targets, preds, squared=False)\n    mae = mean_absolute_error(targets, preds)\n    return np.mean(losses), rmse, mae\n\nall_rmse = []\nall_mae = []\n\n# Assume edge_attr is consistent across graphs and has fixed dim\nedge_dim = graphs_padded[0].edge_attr.shape[1]\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(graphs_padded)):\n    print(f\"Fold {fold + 1}\")\n    train_dataset = [graphs_padded[i] for i in train_idx]\n    val_dataset = [graphs_padded[i] for i in val_idx]\n\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    model = GATGraphRegressor(in_node_feats=max_node_dim, edge_dim=edge_dim).to(device)\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n    best_val_loss = float('inf')\n    patience = 10\n    patience_counter = 0\n\n    for epoch in range(100):\n        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n        val_loss, rmse, mae = eval_model(model, val_loader, criterion)\n        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), f\"best_model_max_fold{fold}.pt\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    model.load_state_dict(torch.load(f\"best_model_max_fold{fold}.pt\"))\n    _, rmse, mae = eval_model(model, val_loader, criterion)\n    all_rmse.append(rmse)\n    all_mae.append(mae)\n    print(f\"Fold {fold + 1} final RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\nprint(f\"\\nAverage RMSE: {np.mean(all_rmse):.4f} ± {np.std(all_rmse):.4f}\")\nprint(f\"Average MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:17:06.308905Z","iopub.execute_input":"2025-07-02T10:17:06.309475Z","iopub.status.idle":"2025-07-02T10:30:51.923037Z","shell.execute_reply.started":"2025-07-02T10:17:06.309448Z","shell.execute_reply":"2025-07-02T10:30:51.922266Z"}},"outputs":[{"name":"stdout","text":"Fold 1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 1.0835 | Val Loss: 0.6919 | RMSE: 0.8318 | MAE: 0.7806\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 1.0709 | Val Loss: 0.7060 | RMSE: 0.8402 | MAE: 0.7886\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 1.0625 | Val Loss: 0.7216 | RMSE: 0.8495 | MAE: 0.7980\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 1.0553 | Val Loss: 0.7267 | RMSE: 0.8525 | MAE: 0.8028\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 1.0483 | Val Loss: 0.7249 | RMSE: 0.8514 | MAE: 0.8016\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 1.0315 | Val Loss: 0.7397 | RMSE: 0.8601 | MAE: 0.8112\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 1.0164 | Val Loss: 0.7362 | RMSE: 0.8580 | MAE: 0.8083\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 1.0062 | Val Loss: 0.7308 | RMSE: 0.8549 | MAE: 0.8069\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 0.9856 | Val Loss: 0.7313 | RMSE: 0.8551 | MAE: 0.8069\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 0.9635 | Val Loss: 0.7206 | RMSE: 0.8489 | MAE: 0.7996\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.9277 | Val Loss: 0.7131 | RMSE: 0.8444 | MAE: 0.7977\nEarly stopping triggered.\nFold 1 final RMSE: 0.8318, MAE: 0.7806\nFold 2\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 1.1104 | Val Loss: 0.5787 | RMSE: 0.7607 | MAE: 0.7247\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 1.1061 | Val Loss: 0.5832 | RMSE: 0.7637 | MAE: 0.7276\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 1.0932 | Val Loss: 0.5821 | RMSE: 0.7629 | MAE: 0.7265\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 1.0875 | Val Loss: 0.5765 | RMSE: 0.7593 | MAE: 0.7227\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 1.0774 | Val Loss: 0.5862 | RMSE: 0.7656 | MAE: 0.7282\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 1.0642 | Val Loss: 0.5818 | RMSE: 0.7628 | MAE: 0.7261\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 1.0484 | Val Loss: 0.5728 | RMSE: 0.7568 | MAE: 0.7203\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 1.0249 | Val Loss: 0.5661 | RMSE: 0.7524 | MAE: 0.7160\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 0.9916 | Val Loss: 0.5585 | RMSE: 0.7473 | MAE: 0.7105\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 0.9462 | Val Loss: 0.5365 | RMSE: 0.7325 | MAE: 0.6951\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.9050 | Val Loss: 0.5223 | RMSE: 0.7227 | MAE: 0.6867\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.8267 | Val Loss: 0.5205 | RMSE: 0.7214 | MAE: 0.6842\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.7546 | Val Loss: 0.4883 | RMSE: 0.6988 | MAE: 0.6631\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.6580 | Val Loss: 0.5055 | RMSE: 0.7110 | MAE: 0.6656\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.5648 | Val Loss: 0.4642 | RMSE: 0.6813 | MAE: 0.6355\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.4629 | Val Loss: 0.4006 | RMSE: 0.6329 | MAE: 0.5870\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.3708 | Val Loss: 0.4193 | RMSE: 0.6475 | MAE: 0.5704\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.3230 | Val Loss: 0.3829 | RMSE: 0.6188 | MAE: 0.5547\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.2519 | Val Loss: 0.3967 | RMSE: 0.6299 | MAE: 0.5527\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.2271 | Val Loss: 0.4384 | RMSE: 0.6621 | MAE: 0.5959\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 0.1856 | Val Loss: 0.4523 | RMSE: 0.6725 | MAE: 0.5705\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 0.1898 | Val Loss: 0.3574 | RMSE: 0.5978 | MAE: 0.5231\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.1776 | Val Loss: 0.3316 | RMSE: 0.5759 | MAE: 0.4931\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 0.1560 | Val Loss: 0.4809 | RMSE: 0.6935 | MAE: 0.6021\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.1496 | Val Loss: 0.4571 | RMSE: 0.6761 | MAE: 0.5908\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.1363 | Val Loss: 0.3459 | RMSE: 0.5881 | MAE: 0.5424\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 | Train Loss: 0.1535 | Val Loss: 0.3562 | RMSE: 0.5968 | MAE: 0.5274\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 | Train Loss: 0.1277 | Val Loss: 0.3818 | RMSE: 0.6179 | MAE: 0.5487\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 | Train Loss: 0.1022 | Val Loss: 0.3877 | RMSE: 0.6227 | MAE: 0.5370\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 | Train Loss: 0.1002 | Val Loss: 0.3954 | RMSE: 0.6288 | MAE: 0.5702\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30 | Train Loss: 0.1284 | Val Loss: 0.3749 | RMSE: 0.6123 | MAE: 0.5433\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31 | Train Loss: 0.0996 | Val Loss: 0.4263 | RMSE: 0.6529 | MAE: 0.5783\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32 | Train Loss: 0.1023 | Val Loss: 0.4310 | RMSE: 0.6565 | MAE: 0.5989\nEarly stopping triggered.\nFold 2 final RMSE: 0.5759, MAE: 0.4931\nFold 3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 0.8717 | Val Loss: 1.5736 | RMSE: 1.2545 | MAE: 0.9416\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.8613 | Val Loss: 1.5689 | RMSE: 1.2526 | MAE: 0.9362\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.8521 | Val Loss: 1.5721 | RMSE: 1.2538 | MAE: 0.9361\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.8473 | Val Loss: 1.5692 | RMSE: 1.2527 | MAE: 0.9332\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.8391 | Val Loss: 1.5649 | RMSE: 1.2509 | MAE: 0.9276\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.8286 | Val Loss: 1.5644 | RMSE: 1.2508 | MAE: 0.9265\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 0.8241 | Val Loss: 1.5613 | RMSE: 1.2495 | MAE: 0.9255\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 0.8172 | Val Loss: 1.5629 | RMSE: 1.2502 | MAE: 0.9245\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 0.8055 | Val Loss: 1.5551 | RMSE: 1.2470 | MAE: 0.9207\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 0.7950 | Val Loss: 1.5504 | RMSE: 1.2452 | MAE: 0.9164\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.7825 | Val Loss: 1.5416 | RMSE: 1.2416 | MAE: 0.9123\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.7554 | Val Loss: 1.5263 | RMSE: 1.2355 | MAE: 0.9094\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.7309 | Val Loss: 1.5088 | RMSE: 1.2283 | MAE: 0.9004\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.7055 | Val Loss: 1.5003 | RMSE: 1.2249 | MAE: 0.8989\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.6778 | Val Loss: 1.4749 | RMSE: 1.2145 | MAE: 0.8848\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.6254 | Val Loss: 1.4427 | RMSE: 1.2011 | MAE: 0.8752\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.5715 | Val Loss: 1.4210 | RMSE: 1.1921 | MAE: 0.8611\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.5159 | Val Loss: 1.3771 | RMSE: 1.1735 | MAE: 0.8541\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.4474 | Val Loss: 1.3105 | RMSE: 1.1448 | MAE: 0.8208\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.3688 | Val Loss: 1.2640 | RMSE: 1.1243 | MAE: 0.7953\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 0.3483 | Val Loss: 1.2087 | RMSE: 1.0994 | MAE: 0.7679\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 0.2807 | Val Loss: 1.1396 | RMSE: 1.0675 | MAE: 0.7675\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.2471 | Val Loss: 1.1272 | RMSE: 1.0617 | MAE: 0.7326\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 0.1761 | Val Loss: 1.0445 | RMSE: 1.0220 | MAE: 0.6846\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.1497 | Val Loss: 1.0816 | RMSE: 1.0400 | MAE: 0.7238\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.1348 | Val Loss: 1.0485 | RMSE: 1.0240 | MAE: 0.7102\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 | Train Loss: 0.1090 | Val Loss: 1.0405 | RMSE: 1.0201 | MAE: 0.7035\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 | Train Loss: 0.1133 | Val Loss: 1.0689 | RMSE: 1.0339 | MAE: 0.7318\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 | Train Loss: 0.0878 | Val Loss: 1.0032 | RMSE: 1.0016 | MAE: 0.6459\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 | Train Loss: 0.0974 | Val Loss: 1.0120 | RMSE: 1.0060 | MAE: 0.6777\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30 | Train Loss: 0.0739 | Val Loss: 1.0217 | RMSE: 1.0108 | MAE: 0.7010\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31 | Train Loss: 0.0740 | Val Loss: 1.0273 | RMSE: 1.0136 | MAE: 0.6746\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32 | Train Loss: 0.0741 | Val Loss: 1.0183 | RMSE: 1.0091 | MAE: 0.6983\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33 | Train Loss: 0.0625 | Val Loss: 0.9807 | RMSE: 0.9903 | MAE: 0.6648\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34 | Train Loss: 0.0623 | Val Loss: 1.0264 | RMSE: 1.0131 | MAE: 0.6771\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35 | Train Loss: 0.0547 | Val Loss: 1.0444 | RMSE: 1.0220 | MAE: 0.7074\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36 | Train Loss: 0.0446 | Val Loss: 1.0127 | RMSE: 1.0064 | MAE: 0.6795\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37 | Train Loss: 0.0492 | Val Loss: 1.0418 | RMSE: 1.0207 | MAE: 0.7095\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38 | Train Loss: 0.0548 | Val Loss: 1.0306 | RMSE: 1.0152 | MAE: 0.7072\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39 | Train Loss: 0.0364 | Val Loss: 1.0060 | RMSE: 1.0030 | MAE: 0.7007\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40 | Train Loss: 0.0330 | Val Loss: 1.0450 | RMSE: 1.0223 | MAE: 0.7131\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41 | Train Loss: 0.0317 | Val Loss: 1.0423 | RMSE: 1.0209 | MAE: 0.6942\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42 | Train Loss: 0.0307 | Val Loss: 1.0135 | RMSE: 1.0067 | MAE: 0.7005\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43 | Train Loss: 0.0390 | Val Loss: 0.9917 | RMSE: 0.9958 | MAE: 0.6744\nEarly stopping triggered.\nFold 3 final RMSE: 0.9903, MAE: 0.6648\nFold 4\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 0.9988 | Val Loss: 1.0904 | RMSE: 1.0442 | MAE: 0.9008\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.9867 | Val Loss: 1.0878 | RMSE: 1.0430 | MAE: 0.9020\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.9826 | Val Loss: 1.0810 | RMSE: 1.0397 | MAE: 0.8991\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.9747 | Val Loss: 1.0820 | RMSE: 1.0402 | MAE: 0.9011\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.9717 | Val Loss: 1.0780 | RMSE: 1.0383 | MAE: 0.9003\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.9656 | Val Loss: 1.0733 | RMSE: 1.0360 | MAE: 0.9007\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 0.9497 | Val Loss: 1.0759 | RMSE: 1.0372 | MAE: 0.8990\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 0.9352 | Val Loss: 1.0725 | RMSE: 1.0356 | MAE: 0.8989\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 0.9251 | Val Loss: 1.0600 | RMSE: 1.0296 | MAE: 0.8921\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 0.9053 | Val Loss: 1.0697 | RMSE: 1.0343 | MAE: 0.8952\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.8736 | Val Loss: 1.0672 | RMSE: 1.0330 | MAE: 0.8914\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.8538 | Val Loss: 1.0513 | RMSE: 1.0253 | MAE: 0.8840\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.8211 | Val Loss: 1.0358 | RMSE: 1.0178 | MAE: 0.8800\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.7890 | Val Loss: 1.0285 | RMSE: 1.0141 | MAE: 0.8687\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.7491 | Val Loss: 1.0157 | RMSE: 1.0078 | MAE: 0.8622\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.6855 | Val Loss: 0.9796 | RMSE: 0.9898 | MAE: 0.8493\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.6360 | Val Loss: 0.9474 | RMSE: 0.9734 | MAE: 0.8309\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.5841 | Val Loss: 0.9238 | RMSE: 0.9611 | MAE: 0.8162\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.5222 | Val Loss: 0.8860 | RMSE: 0.9413 | MAE: 0.8046\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.4463 | Val Loss: 0.8453 | RMSE: 0.9194 | MAE: 0.7689\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 0.3805 | Val Loss: 0.8409 | RMSE: 0.9170 | MAE: 0.7762\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 0.3311 | Val Loss: 0.7803 | RMSE: 0.8833 | MAE: 0.7310\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.2792 | Val Loss: 0.7322 | RMSE: 0.8557 | MAE: 0.7254\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 0.2585 | Val Loss: 0.7541 | RMSE: 0.8684 | MAE: 0.7502\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.2641 | Val Loss: 0.7637 | RMSE: 0.8739 | MAE: 0.7314\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.2324 | Val Loss: 0.7145 | RMSE: 0.8453 | MAE: 0.6846\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 | Train Loss: 0.2098 | Val Loss: 0.6924 | RMSE: 0.8321 | MAE: 0.6970\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 | Train Loss: 0.1729 | Val Loss: 0.6451 | RMSE: 0.8032 | MAE: 0.6622\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 | Train Loss: 0.1650 | Val Loss: 0.7241 | RMSE: 0.8510 | MAE: 0.6986\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 | Train Loss: 0.1829 | Val Loss: 0.7230 | RMSE: 0.8503 | MAE: 0.7196\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30 | Train Loss: 0.1724 | Val Loss: 0.6989 | RMSE: 0.8360 | MAE: 0.6943\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31 | Train Loss: 0.1307 | Val Loss: 0.6818 | RMSE: 0.8257 | MAE: 0.6934\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32 | Train Loss: 0.1514 | Val Loss: 0.6702 | RMSE: 0.8186 | MAE: 0.6807\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33 | Train Loss: 0.1054 | Val Loss: 0.6820 | RMSE: 0.8258 | MAE: 0.6765\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34 | Train Loss: 0.1316 | Val Loss: 0.6946 | RMSE: 0.8334 | MAE: 0.6915\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35 | Train Loss: 0.0909 | Val Loss: 0.6887 | RMSE: 0.8299 | MAE: 0.6928\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36 | Train Loss: 0.1022 | Val Loss: 0.6689 | RMSE: 0.8179 | MAE: 0.6736\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37 | Train Loss: 0.0909 | Val Loss: 0.7175 | RMSE: 0.8471 | MAE: 0.6947\nEarly stopping triggered.\nFold 4 final RMSE: 0.8032, MAE: 0.6622\nFold 5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 1.0174 | Val Loss: 1.0514 | RMSE: 1.0254 | MAE: 0.8687\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.9884 | Val Loss: 1.0807 | RMSE: 1.0396 | MAE: 0.8783\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.9691 | Val Loss: 1.1004 | RMSE: 1.0490 | MAE: 0.8845\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.9557 | Val Loss: 1.1136 | RMSE: 1.0553 | MAE: 0.8887\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.9469 | Val Loss: 1.1358 | RMSE: 1.0657 | MAE: 0.8955\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.9328 | Val Loss: 1.1465 | RMSE: 1.0708 | MAE: 0.8981\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 0.9238 | Val Loss: 1.1460 | RMSE: 1.0705 | MAE: 0.8963\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 0.9142 | Val Loss: 1.1543 | RMSE: 1.0744 | MAE: 0.8983\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 0.8945 | Val Loss: 1.1667 | RMSE: 1.0801 | MAE: 0.9012\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 0.8770 | Val Loss: 1.1610 | RMSE: 1.0775 | MAE: 0.8967\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(message)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.8525 | Val Loss: 1.1731 | RMSE: 1.0831 | MAE: 0.8969\nEarly stopping triggered.\nFold 5 final RMSE: 1.0254, MAE: 0.8687\n\nAverage RMSE: 0.8453 ± 0.1600\nAverage MAE: 0.6939 ± 0.1267\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Use now globalAttention","metadata":{"execution":{"iopub.status.busy":"2025-05-23T00:18:32.091419Z","iopub.execute_input":"2025-05-23T00:18:32.092461Z","iopub.status.idle":"2025-05-23T00:18:32.107241Z","shell.execute_reply.started":"2025-05-23T00:18:32.092378Z","shell.execute_reply":"2025-05-23T00:18:32.105432Z"}}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv, GlobalAttention\nfrom torch_geometric.loader import DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport numpy as np\n\nclass GATGraphRegressor(torch.nn.Module):\n    def __init__(self, in_node_feats, edge_feat_dim, hidden_dim=64, heads=4):\n        super(GATGraphRegressor, self).__init__()\n\n        self.gat1 = GATConv(in_node_feats, hidden_dim, heads=heads, concat=True, dropout=0.2)\n        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=True, dropout=0.2)\n        \n        self.att_pool = GlobalAttention(gate_nn=torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, 1),\n            torch.nn.Sigmoid()\n        ))\n        \n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n\n        # Optional: concatenate edge_attr info into node features or apply custom logic\n        # Currently unused because GATConv does not support edge_attr directly\n\n        x = self.gat1(x, edge_index)\n        x = F.elu(x)\n        x = self.gat2(x, edge_index)\n        x = F.elu(x)\n        x = self.att_pool(x, batch)\n        return self.mlp(x).squeeze(1)\n\n# Normalize targets\nys_all = [g.y.item() for g in graphs_padded]\nmean_y = np.mean(ys_all)\nstd_y = np.std(ys_all)\n\nfor g in graphs_padded:\n    g.y = torch.tensor([(g.y.item() - mean_y) / std_y], dtype=torch.float)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\ndef train_epoch(model, loader, criterion, optimizer):\n    model.train()\n    losses = []\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y.view(-1))\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    return np.mean(losses)\n\ndef eval_model(model, loader, criterion):\n    model.eval()\n    losses = []\n    preds, targets = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y.view(-1))\n            losses.append(loss.item())\n            preds.extend(out.cpu().numpy())\n            targets.extend(batch.y.view(-1).cpu().numpy())\n    rmse = mean_squared_error(targets, preds, squared=False)\n    mae = mean_absolute_error(targets, preds)\n    return np.mean(losses), rmse, mae\n\nall_rmse = []\nall_mae = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(graphs_padded)):\n    print(f\"Fold {fold + 1}\")\n    train_dataset = [graphs_padded[i] for i in train_idx]\n    val_dataset = [graphs_padded[i] for i in val_idx]\n\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    model = GATGraphRegressor(in_node_feats=max_node_dim, edge_feat_dim=max_edge_dim).to(device)\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n    best_val_loss = float('inf')\n    patience = 10\n    patience_counter = 0\n\n    for epoch in range(100):\n        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n        val_loss, rmse, mae = eval_model(model, val_loader, criterion)\n        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), f\"best_model_ga_fold{fold}.pt\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    model.load_state_dict(torch.load(f\"best_model_ga_fold{fold}.pt\"))\n    _, rmse, mae = eval_model(model, val_loader, criterion)\n    all_rmse.append(rmse)\n    all_mae.append(mae)\n    print(f\"Fold {fold + 1} final RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n\nprint(f\"\\nAverage RMSE: {np.mean(all_rmse):.4f} ± {np.std(all_rmse):.4f}\")\nprint(f\"Average MAE: {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:30:51.923956Z","iopub.execute_input":"2025-07-02T10:30:51.924273Z","iopub.status.idle":"2025-07-02T10:47:08.337322Z","shell.execute_reply.started":"2025-07-02T10:30:51.924241Z","shell.execute_reply":"2025-07-02T10:47:08.336507Z"}},"outputs":[{"name":"stdout","text":"Fold 1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 1.0664 | Val Loss: 0.7580 | RMSE: 0.8706 | MAE: 0.8221\nEpoch 1 | Train Loss: 1.0574 | Val Loss: 0.7522 | RMSE: 0.8673 | MAE: 0.8192\nEpoch 2 | Train Loss: 1.0307 | Val Loss: 0.7331 | RMSE: 0.8562 | MAE: 0.8093\nEpoch 3 | Train Loss: 0.9615 | Val Loss: 0.6572 | RMSE: 0.8107 | MAE: 0.7651\nEpoch 4 | Train Loss: 0.8377 | Val Loss: 0.5105 | RMSE: 0.7145 | MAE: 0.6648\nEpoch 5 | Train Loss: 0.6858 | Val Loss: 0.3551 | RMSE: 0.5959 | MAE: 0.5230\nEpoch 6 | Train Loss: 0.5189 | Val Loss: 0.2659 | RMSE: 0.5157 | MAE: 0.4681\nEpoch 7 | Train Loss: 0.4044 | Val Loss: 0.1962 | RMSE: 0.4430 | MAE: 0.3661\nEpoch 8 | Train Loss: 0.3519 | Val Loss: 0.1937 | RMSE: 0.4402 | MAE: 0.3667\nEpoch 9 | Train Loss: 0.3321 | Val Loss: 0.1747 | RMSE: 0.4180 | MAE: 0.3251\nEpoch 10 | Train Loss: 0.3308 | Val Loss: 0.1724 | RMSE: 0.4152 | MAE: 0.3000\nEpoch 11 | Train Loss: 0.3171 | Val Loss: 0.1739 | RMSE: 0.4170 | MAE: 0.3046\nEpoch 12 | Train Loss: 0.3170 | Val Loss: 0.1760 | RMSE: 0.4195 | MAE: 0.3242\nEpoch 13 | Train Loss: 0.3103 | Val Loss: 0.1754 | RMSE: 0.4189 | MAE: 0.2993\nEpoch 14 | Train Loss: 0.3005 | Val Loss: 0.1925 | RMSE: 0.4387 | MAE: 0.2682\nEpoch 15 | Train Loss: 0.3064 | Val Loss: 0.1797 | RMSE: 0.4239 | MAE: 0.2887\nEpoch 16 | Train Loss: 0.2975 | Val Loss: 0.1781 | RMSE: 0.4220 | MAE: 0.2974\nEpoch 17 | Train Loss: 0.2999 | Val Loss: 0.1782 | RMSE: 0.4222 | MAE: 0.3082\nEpoch 18 | Train Loss: 0.2909 | Val Loss: 0.1750 | RMSE: 0.4183 | MAE: 0.3028\nEpoch 19 | Train Loss: 0.2858 | Val Loss: 0.1744 | RMSE: 0.4176 | MAE: 0.3031\nEpoch 20 | Train Loss: 0.2814 | Val Loss: 0.1833 | RMSE: 0.4281 | MAE: 0.2743\nEarly stopping triggered.\nFold 1 final RMSE: 0.4152, MAE: 0.3000\nFold 2\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 1.1070 | Val Loss: 0.6033 | RMSE: 0.7767 | MAE: 0.7379\nEpoch 1 | Train Loss: 1.0982 | Val Loss: 0.5870 | RMSE: 0.7661 | MAE: 0.7284\nEpoch 2 | Train Loss: 1.0753 | Val Loss: 0.5502 | RMSE: 0.7417 | MAE: 0.7062\nEpoch 3 | Train Loss: 1.0101 | Val Loss: 0.4909 | RMSE: 0.7007 | MAE: 0.6656\nEpoch 4 | Train Loss: 0.8767 | Val Loss: 0.3752 | RMSE: 0.6125 | MAE: 0.5788\nEpoch 5 | Train Loss: 0.7120 | Val Loss: 0.2781 | RMSE: 0.5273 | MAE: 0.4844\nEpoch 6 | Train Loss: 0.5225 | Val Loss: 0.2041 | RMSE: 0.4518 | MAE: 0.4057\nEpoch 7 | Train Loss: 0.3797 | Val Loss: 0.2300 | RMSE: 0.4796 | MAE: 0.3811\nEpoch 8 | Train Loss: 0.3224 | Val Loss: 0.2355 | RMSE: 0.4853 | MAE: 0.3367\nEpoch 9 | Train Loss: 0.3087 | Val Loss: 0.2548 | RMSE: 0.5048 | MAE: 0.3427\nEpoch 10 | Train Loss: 0.3000 | Val Loss: 0.2720 | RMSE: 0.5215 | MAE: 0.3460\nEpoch 11 | Train Loss: 0.2986 | Val Loss: 0.2636 | RMSE: 0.5135 | MAE: 0.3333\nEpoch 12 | Train Loss: 0.2936 | Val Loss: 0.2779 | RMSE: 0.5271 | MAE: 0.3422\nEpoch 13 | Train Loss: 0.2814 | Val Loss: 0.2982 | RMSE: 0.5460 | MAE: 0.3565\nEpoch 14 | Train Loss: 0.2904 | Val Loss: 0.2711 | RMSE: 0.5207 | MAE: 0.3348\nEpoch 15 | Train Loss: 0.2785 | Val Loss: 0.3032 | RMSE: 0.5506 | MAE: 0.3642\nEpoch 16 | Train Loss: 0.2698 | Val Loss: 0.3104 | RMSE: 0.5572 | MAE: 0.3667\nEarly stopping triggered.\nFold 2 final RMSE: 0.4518, MAE: 0.4057\nFold 3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 0.8640 | Val Loss: 1.5688 | RMSE: 1.2525 | MAE: 0.9326\nEpoch 1 | Train Loss: 0.8559 | Val Loss: 1.5576 | RMSE: 1.2480 | MAE: 0.9248\nEpoch 2 | Train Loss: 0.8388 | Val Loss: 1.5296 | RMSE: 1.2368 | MAE: 0.9113\nEpoch 3 | Train Loss: 0.8019 | Val Loss: 1.4718 | RMSE: 1.2132 | MAE: 0.8830\nEpoch 4 | Train Loss: 0.7195 | Val Loss: 1.3286 | RMSE: 1.1527 | MAE: 0.8097\nEpoch 5 | Train Loss: 0.5638 | Val Loss: 1.1175 | RMSE: 1.0571 | MAE: 0.6932\nEpoch 6 | Train Loss: 0.4048 | Val Loss: 0.9432 | RMSE: 0.9712 | MAE: 0.6086\nEpoch 7 | Train Loss: 0.2928 | Val Loss: 0.8137 | RMSE: 0.9020 | MAE: 0.5246\nEpoch 8 | Train Loss: 0.2397 | Val Loss: 0.7590 | RMSE: 0.8712 | MAE: 0.4880\nEpoch 9 | Train Loss: 0.2174 | Val Loss: 0.7399 | RMSE: 0.8602 | MAE: 0.5237\nEpoch 10 | Train Loss: 0.2099 | Val Loss: 0.7138 | RMSE: 0.8449 | MAE: 0.5078\nEpoch 11 | Train Loss: 0.2097 | Val Loss: 0.7091 | RMSE: 0.8421 | MAE: 0.5162\nEpoch 12 | Train Loss: 0.1995 | Val Loss: 0.6951 | RMSE: 0.8337 | MAE: 0.5261\nEpoch 13 | Train Loss: 0.1952 | Val Loss: 0.6769 | RMSE: 0.8227 | MAE: 0.4686\nEpoch 14 | Train Loss: 0.1922 | Val Loss: 0.6744 | RMSE: 0.8212 | MAE: 0.4954\nEpoch 15 | Train Loss: 0.1910 | Val Loss: 0.6660 | RMSE: 0.8161 | MAE: 0.4819\nEpoch 16 | Train Loss: 0.1857 | Val Loss: 0.6643 | RMSE: 0.8150 | MAE: 0.4866\nEpoch 17 | Train Loss: 0.1851 | Val Loss: 0.6558 | RMSE: 0.8098 | MAE: 0.4595\nEpoch 18 | Train Loss: 0.1837 | Val Loss: 0.6553 | RMSE: 0.8095 | MAE: 0.4813\nEpoch 19 | Train Loss: 0.1802 | Val Loss: 0.6574 | RMSE: 0.8108 | MAE: 0.4899\nEpoch 20 | Train Loss: 0.1778 | Val Loss: 0.6655 | RMSE: 0.8158 | MAE: 0.5118\nEpoch 21 | Train Loss: 0.1757 | Val Loss: 0.6449 | RMSE: 0.8030 | MAE: 0.4786\nEpoch 22 | Train Loss: 0.1735 | Val Loss: 0.6429 | RMSE: 0.8018 | MAE: 0.4859\nEpoch 23 | Train Loss: 0.1701 | Val Loss: 0.6374 | RMSE: 0.7984 | MAE: 0.4690\nEpoch 24 | Train Loss: 0.1659 | Val Loss: 0.6365 | RMSE: 0.7978 | MAE: 0.4785\nEpoch 25 | Train Loss: 0.1649 | Val Loss: 0.6443 | RMSE: 0.8027 | MAE: 0.5003\nEpoch 26 | Train Loss: 0.1677 | Val Loss: 0.6432 | RMSE: 0.8020 | MAE: 0.5090\nEpoch 27 | Train Loss: 0.1620 | Val Loss: 0.6260 | RMSE: 0.7912 | MAE: 0.4597\nEpoch 28 | Train Loss: 0.1613 | Val Loss: 0.6285 | RMSE: 0.7928 | MAE: 0.4511\nEpoch 29 | Train Loss: 0.1589 | Val Loss: 0.6311 | RMSE: 0.7944 | MAE: 0.4537\nEpoch 30 | Train Loss: 0.1562 | Val Loss: 0.6235 | RMSE: 0.7896 | MAE: 0.4750\nEpoch 31 | Train Loss: 0.1535 | Val Loss: 0.6204 | RMSE: 0.7877 | MAE: 0.4672\nEpoch 32 | Train Loss: 0.1456 | Val Loss: 0.6146 | RMSE: 0.7840 | MAE: 0.4530\nEpoch 33 | Train Loss: 0.1451 | Val Loss: 0.6076 | RMSE: 0.7795 | MAE: 0.4673\nEpoch 34 | Train Loss: 0.1391 | Val Loss: 0.6035 | RMSE: 0.7769 | MAE: 0.4513\nEpoch 35 | Train Loss: 0.1379 | Val Loss: 0.6078 | RMSE: 0.7796 | MAE: 0.4829\nEpoch 36 | Train Loss: 0.1341 | Val Loss: 0.5982 | RMSE: 0.7735 | MAE: 0.4556\nEpoch 37 | Train Loss: 0.1323 | Val Loss: 0.6006 | RMSE: 0.7750 | MAE: 0.4673\nEpoch 38 | Train Loss: 0.1388 | Val Loss: 0.6035 | RMSE: 0.7768 | MAE: 0.4860\nEpoch 39 | Train Loss: 0.1283 | Val Loss: 0.5967 | RMSE: 0.7724 | MAE: 0.4759\nEpoch 40 | Train Loss: 0.1197 | Val Loss: 0.6071 | RMSE: 0.7792 | MAE: 0.5084\nEpoch 41 | Train Loss: 0.1267 | Val Loss: 0.5790 | RMSE: 0.7609 | MAE: 0.4388\nEpoch 42 | Train Loss: 0.1194 | Val Loss: 0.5767 | RMSE: 0.7594 | MAE: 0.4510\nEpoch 43 | Train Loss: 0.1168 | Val Loss: 0.5864 | RMSE: 0.7657 | MAE: 0.4771\nEpoch 44 | Train Loss: 0.1128 | Val Loss: 0.5760 | RMSE: 0.7589 | MAE: 0.4401\nEpoch 45 | Train Loss: 0.1146 | Val Loss: 0.5680 | RMSE: 0.7536 | MAE: 0.4500\nEpoch 46 | Train Loss: 0.1114 | Val Loss: 0.5683 | RMSE: 0.7539 | MAE: 0.4624\nEpoch 47 | Train Loss: 0.1071 | Val Loss: 0.5732 | RMSE: 0.7571 | MAE: 0.4753\nEpoch 48 | Train Loss: 0.1056 | Val Loss: 0.5692 | RMSE: 0.7544 | MAE: 0.4726\nEpoch 49 | Train Loss: 0.1016 | Val Loss: 0.5706 | RMSE: 0.7554 | MAE: 0.4768\nEpoch 50 | Train Loss: 0.1018 | Val Loss: 0.5533 | RMSE: 0.7439 | MAE: 0.4424\nEpoch 51 | Train Loss: 0.0974 | Val Loss: 0.5497 | RMSE: 0.7414 | MAE: 0.4472\nEpoch 52 | Train Loss: 0.1010 | Val Loss: 0.5437 | RMSE: 0.7374 | MAE: 0.4327\nEpoch 53 | Train Loss: 0.0922 | Val Loss: 0.5405 | RMSE: 0.7352 | MAE: 0.4441\nEpoch 54 | Train Loss: 0.0863 | Val Loss: 0.5358 | RMSE: 0.7320 | MAE: 0.4230\nEpoch 55 | Train Loss: 0.0837 | Val Loss: 0.5528 | RMSE: 0.7435 | MAE: 0.4818\nEpoch 56 | Train Loss: 0.0865 | Val Loss: 0.5277 | RMSE: 0.7264 | MAE: 0.4430\nEpoch 57 | Train Loss: 0.0913 | Val Loss: 0.5339 | RMSE: 0.7307 | MAE: 0.4532\nEpoch 58 | Train Loss: 0.0772 | Val Loss: 0.5313 | RMSE: 0.7289 | MAE: 0.4608\nEpoch 59 | Train Loss: 0.0756 | Val Loss: 0.5195 | RMSE: 0.7208 | MAE: 0.4272\nEpoch 60 | Train Loss: 0.0733 | Val Loss: 0.5317 | RMSE: 0.7292 | MAE: 0.4679\nEpoch 61 | Train Loss: 0.0690 | Val Loss: 0.5330 | RMSE: 0.7301 | MAE: 0.4730\nEpoch 62 | Train Loss: 0.0666 | Val Loss: 0.5088 | RMSE: 0.7133 | MAE: 0.4329\nEpoch 63 | Train Loss: 0.0637 | Val Loss: 0.5066 | RMSE: 0.7118 | MAE: 0.4327\nEpoch 64 | Train Loss: 0.0577 | Val Loss: 0.5185 | RMSE: 0.7201 | MAE: 0.4662\nEpoch 65 | Train Loss: 0.0597 | Val Loss: 0.4940 | RMSE: 0.7029 | MAE: 0.4301\nEpoch 66 | Train Loss: 0.0520 | Val Loss: 0.5001 | RMSE: 0.7072 | MAE: 0.4450\nEpoch 67 | Train Loss: 0.0496 | Val Loss: 0.4870 | RMSE: 0.6979 | MAE: 0.4222\nEpoch 68 | Train Loss: 0.0495 | Val Loss: 0.4787 | RMSE: 0.6919 | MAE: 0.4225\nEpoch 69 | Train Loss: 0.0458 | Val Loss: 0.4748 | RMSE: 0.6891 | MAE: 0.4243\nEpoch 70 | Train Loss: 0.0396 | Val Loss: 0.4720 | RMSE: 0.6871 | MAE: 0.4145\nEpoch 71 | Train Loss: 0.0396 | Val Loss: 0.4850 | RMSE: 0.6964 | MAE: 0.4459\nEpoch 72 | Train Loss: 0.0362 | Val Loss: 0.4691 | RMSE: 0.6849 | MAE: 0.4261\nEpoch 73 | Train Loss: 0.0326 | Val Loss: 0.4691 | RMSE: 0.6849 | MAE: 0.4376\nEpoch 74 | Train Loss: 0.0303 | Val Loss: 0.4783 | RMSE: 0.6916 | MAE: 0.4576\nEpoch 75 | Train Loss: 0.0297 | Val Loss: 0.4544 | RMSE: 0.6741 | MAE: 0.4151\nEpoch 76 | Train Loss: 0.0304 | Val Loss: 0.4776 | RMSE: 0.6911 | MAE: 0.4669\nEpoch 77 | Train Loss: 0.0262 | Val Loss: 0.4442 | RMSE: 0.6665 | MAE: 0.4153\nEpoch 78 | Train Loss: 0.0260 | Val Loss: 0.4544 | RMSE: 0.6741 | MAE: 0.4406\nEpoch 79 | Train Loss: 0.0226 | Val Loss: 0.4375 | RMSE: 0.6614 | MAE: 0.4227\nEpoch 80 | Train Loss: 0.0252 | Val Loss: 0.4430 | RMSE: 0.6656 | MAE: 0.4342\nEpoch 81 | Train Loss: 0.0205 | Val Loss: 0.4456 | RMSE: 0.6675 | MAE: 0.4412\nEpoch 82 | Train Loss: 0.0198 | Val Loss: 0.4439 | RMSE: 0.6663 | MAE: 0.4423\nEpoch 83 | Train Loss: 0.0189 | Val Loss: 0.4407 | RMSE: 0.6639 | MAE: 0.4413\nEpoch 84 | Train Loss: 0.0169 | Val Loss: 0.4560 | RMSE: 0.6753 | MAE: 0.4658\nEpoch 85 | Train Loss: 0.0168 | Val Loss: 0.4275 | RMSE: 0.6539 | MAE: 0.4262\nEpoch 86 | Train Loss: 0.0172 | Val Loss: 0.4264 | RMSE: 0.6530 | MAE: 0.4274\nEpoch 87 | Train Loss: 0.0154 | Val Loss: 0.4197 | RMSE: 0.6479 | MAE: 0.4188\nEpoch 88 | Train Loss: 0.0166 | Val Loss: 0.4201 | RMSE: 0.6482 | MAE: 0.4267\nEpoch 89 | Train Loss: 0.0162 | Val Loss: 0.4216 | RMSE: 0.6493 | MAE: 0.4331\nEpoch 90 | Train Loss: 0.0135 | Val Loss: 0.4189 | RMSE: 0.6472 | MAE: 0.4281\nEpoch 91 | Train Loss: 0.0140 | Val Loss: 0.4171 | RMSE: 0.6459 | MAE: 0.4293\nEpoch 92 | Train Loss: 0.0134 | Val Loss: 0.4169 | RMSE: 0.6457 | MAE: 0.4289\nEpoch 93 | Train Loss: 0.0110 | Val Loss: 0.4161 | RMSE: 0.6451 | MAE: 0.4316\nEpoch 94 | Train Loss: 0.0112 | Val Loss: 0.4147 | RMSE: 0.6440 | MAE: 0.4319\nEpoch 95 | Train Loss: 0.0111 | Val Loss: 0.4422 | RMSE: 0.6650 | MAE: 0.4686\nEpoch 96 | Train Loss: 0.0116 | Val Loss: 0.4142 | RMSE: 0.6436 | MAE: 0.4350\nEpoch 97 | Train Loss: 0.0102 | Val Loss: 0.4052 | RMSE: 0.6365 | MAE: 0.4235\nEpoch 98 | Train Loss: 0.0081 | Val Loss: 0.4101 | RMSE: 0.6404 | MAE: 0.4285\nEpoch 99 | Train Loss: 0.0084 | Val Loss: 0.4089 | RMSE: 0.6394 | MAE: 0.4333\nFold 3 final RMSE: 0.6365, MAE: 0.4235\nFold 4\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 0.9895 | Val Loss: 1.0387 | RMSE: 1.0192 | MAE: 0.9011\nEpoch 1 | Train Loss: 0.9832 | Val Loss: 1.0381 | RMSE: 1.0189 | MAE: 0.8963\nEpoch 2 | Train Loss: 0.9623 | Val Loss: 1.0109 | RMSE: 1.0054 | MAE: 0.8832\nEpoch 3 | Train Loss: 0.9039 | Val Loss: 0.9202 | RMSE: 0.9593 | MAE: 0.8402\nEpoch 4 | Train Loss: 0.7818 | Val Loss: 0.7170 | RMSE: 0.8467 | MAE: 0.7476\nEpoch 5 | Train Loss: 0.6250 | Val Loss: 0.5301 | RMSE: 0.7281 | MAE: 0.6345\nEpoch 6 | Train Loss: 0.4633 | Val Loss: 0.4198 | RMSE: 0.6479 | MAE: 0.5364\nEpoch 7 | Train Loss: 0.3689 | Val Loss: 0.3437 | RMSE: 0.5863 | MAE: 0.4487\nEpoch 8 | Train Loss: 0.3188 | Val Loss: 0.2655 | RMSE: 0.5153 | MAE: 0.3727\nEpoch 9 | Train Loss: 0.3023 | Val Loss: 0.2424 | RMSE: 0.4923 | MAE: 0.3574\nEpoch 10 | Train Loss: 0.2929 | Val Loss: 0.2461 | RMSE: 0.4961 | MAE: 0.3523\nEpoch 11 | Train Loss: 0.2910 | Val Loss: 0.2331 | RMSE: 0.4828 | MAE: 0.3456\nEpoch 12 | Train Loss: 0.2846 | Val Loss: 0.2963 | RMSE: 0.5443 | MAE: 0.3794\nEpoch 13 | Train Loss: 0.2792 | Val Loss: 0.2883 | RMSE: 0.5369 | MAE: 0.3742\nEpoch 14 | Train Loss: 0.2732 | Val Loss: 0.2795 | RMSE: 0.5287 | MAE: 0.3683\nEpoch 15 | Train Loss: 0.2722 | Val Loss: 0.2482 | RMSE: 0.4982 | MAE: 0.3503\nEpoch 16 | Train Loss: 0.2678 | Val Loss: 0.2882 | RMSE: 0.5368 | MAE: 0.3737\nEpoch 17 | Train Loss: 0.2707 | Val Loss: 0.2448 | RMSE: 0.4948 | MAE: 0.3445\nEpoch 18 | Train Loss: 0.2582 | Val Loss: 0.2770 | RMSE: 0.5263 | MAE: 0.3659\nEpoch 19 | Train Loss: 0.2516 | Val Loss: 0.2473 | RMSE: 0.4973 | MAE: 0.3469\nEpoch 20 | Train Loss: 0.2541 | Val Loss: 0.2293 | RMSE: 0.4789 | MAE: 0.3427\nEpoch 21 | Train Loss: 0.2489 | Val Loss: 0.2955 | RMSE: 0.5436 | MAE: 0.3770\nEpoch 22 | Train Loss: 0.2450 | Val Loss: 0.2632 | RMSE: 0.5131 | MAE: 0.3570\nEpoch 23 | Train Loss: 0.2391 | Val Loss: 0.2788 | RMSE: 0.5280 | MAE: 0.3668\nEpoch 24 | Train Loss: 0.2358 | Val Loss: 0.3359 | RMSE: 0.5796 | MAE: 0.4004\nEpoch 25 | Train Loss: 0.2542 | Val Loss: 0.3039 | RMSE: 0.5513 | MAE: 0.3819\nEpoch 26 | Train Loss: 0.2300 | Val Loss: 0.2904 | RMSE: 0.5389 | MAE: 0.3740\nEpoch 27 | Train Loss: 0.2241 | Val Loss: 0.2760 | RMSE: 0.5254 | MAE: 0.3651\nEpoch 28 | Train Loss: 0.2163 | Val Loss: 0.2653 | RMSE: 0.5151 | MAE: 0.3580\nEpoch 29 | Train Loss: 0.2091 | Val Loss: 0.3120 | RMSE: 0.5585 | MAE: 0.3877\nEpoch 30 | Train Loss: 0.2036 | Val Loss: 0.2215 | RMSE: 0.4707 | MAE: 0.3427\nEpoch 31 | Train Loss: 0.2092 | Val Loss: 0.2690 | RMSE: 0.5187 | MAE: 0.3606\nEpoch 32 | Train Loss: 0.2029 | Val Loss: 0.3398 | RMSE: 0.5829 | MAE: 0.4028\nEpoch 33 | Train Loss: 0.1980 | Val Loss: 0.2768 | RMSE: 0.5261 | MAE: 0.3658\nEpoch 34 | Train Loss: 0.1960 | Val Loss: 0.3691 | RMSE: 0.6075 | MAE: 0.4292\nEpoch 35 | Train Loss: 0.2025 | Val Loss: 0.3302 | RMSE: 0.5746 | MAE: 0.3982\nEpoch 36 | Train Loss: 0.1897 | Val Loss: 0.2665 | RMSE: 0.5163 | MAE: 0.3588\nEpoch 37 | Train Loss: 0.1942 | Val Loss: 0.3378 | RMSE: 0.5812 | MAE: 0.4040\nEpoch 38 | Train Loss: 0.1809 | Val Loss: 0.2742 | RMSE: 0.5236 | MAE: 0.3647\nEpoch 39 | Train Loss: 0.1738 | Val Loss: 0.2415 | RMSE: 0.4914 | MAE: 0.3445\nEpoch 40 | Train Loss: 0.1702 | Val Loss: 0.2415 | RMSE: 0.4914 | MAE: 0.3422\nEarly stopping triggered.\nFold 4 final RMSE: 0.4707, MAE: 0.3427\nFold 5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n  warnings.warn(out)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 | Train Loss: 0.9572 | Val Loss: 1.2225 | RMSE: 1.1057 | MAE: 0.9179\nEpoch 1 | Train Loss: 0.9503 | Val Loss: 1.2125 | RMSE: 1.1012 | MAE: 0.9140\nEpoch 2 | Train Loss: 0.9413 | Val Loss: 1.2106 | RMSE: 1.1003 | MAE: 0.9092\nEpoch 3 | Train Loss: 0.9176 | Val Loss: 1.1739 | RMSE: 1.0835 | MAE: 0.8886\nEpoch 4 | Train Loss: 0.8646 | Val Loss: 1.0834 | RMSE: 1.0408 | MAE: 0.8387\nEpoch 5 | Train Loss: 0.7483 | Val Loss: 0.8713 | RMSE: 0.9334 | MAE: 0.7314\nEpoch 6 | Train Loss: 0.6145 | Val Loss: 0.7090 | RMSE: 0.8420 | MAE: 0.6240\nEpoch 7 | Train Loss: 0.5048 | Val Loss: 0.5113 | RMSE: 0.7150 | MAE: 0.4951\nEpoch 8 | Train Loss: 0.4100 | Val Loss: 0.3932 | RMSE: 0.6270 | MAE: 0.3838\nEpoch 9 | Train Loss: 0.3659 | Val Loss: 0.3178 | RMSE: 0.5637 | MAE: 0.3245\nEpoch 10 | Train Loss: 0.3400 | Val Loss: 0.2902 | RMSE: 0.5387 | MAE: 0.3108\nEpoch 11 | Train Loss: 0.3233 | Val Loss: 0.2678 | RMSE: 0.5175 | MAE: 0.2927\nEpoch 12 | Train Loss: 0.3198 | Val Loss: 0.2708 | RMSE: 0.5204 | MAE: 0.3086\nEpoch 13 | Train Loss: 0.3096 | Val Loss: 0.2375 | RMSE: 0.4873 | MAE: 0.2599\nEpoch 14 | Train Loss: 0.3027 | Val Loss: 0.2313 | RMSE: 0.4809 | MAE: 0.2655\nEpoch 15 | Train Loss: 0.2959 | Val Loss: 0.2314 | RMSE: 0.4810 | MAE: 0.2671\nEpoch 16 | Train Loss: 0.2914 | Val Loss: 0.2315 | RMSE: 0.4811 | MAE: 0.2754\nEpoch 17 | Train Loss: 0.2844 | Val Loss: 0.2272 | RMSE: 0.4766 | MAE: 0.2656\nEpoch 18 | Train Loss: 0.2862 | Val Loss: 0.2289 | RMSE: 0.4785 | MAE: 0.2780\nEpoch 19 | Train Loss: 0.2800 | Val Loss: 0.2201 | RMSE: 0.4691 | MAE: 0.2750\nEpoch 20 | Train Loss: 0.2778 | Val Loss: 0.2197 | RMSE: 0.4687 | MAE: 0.2773\nEpoch 21 | Train Loss: 0.2780 | Val Loss: 0.2176 | RMSE: 0.4664 | MAE: 0.2820\nEpoch 22 | Train Loss: 0.2706 | Val Loss: 0.2245 | RMSE: 0.4739 | MAE: 0.2863\nEpoch 23 | Train Loss: 0.2692 | Val Loss: 0.2157 | RMSE: 0.4645 | MAE: 0.2777\nEpoch 24 | Train Loss: 0.2657 | Val Loss: 0.2213 | RMSE: 0.4704 | MAE: 0.2832\nEpoch 25 | Train Loss: 0.2640 | Val Loss: 0.2211 | RMSE: 0.4702 | MAE: 0.2961\nEpoch 26 | Train Loss: 0.2633 | Val Loss: 0.2150 | RMSE: 0.4637 | MAE: 0.2833\nEpoch 27 | Train Loss: 0.2543 | Val Loss: 0.2121 | RMSE: 0.4606 | MAE: 0.2817\nEpoch 28 | Train Loss: 0.2502 | Val Loss: 0.2121 | RMSE: 0.4606 | MAE: 0.2902\nEpoch 29 | Train Loss: 0.2485 | Val Loss: 0.2103 | RMSE: 0.4586 | MAE: 0.2864\nEpoch 30 | Train Loss: 0.2427 | Val Loss: 0.2099 | RMSE: 0.4582 | MAE: 0.2888\nEpoch 31 | Train Loss: 0.2441 | Val Loss: 0.2101 | RMSE: 0.4584 | MAE: 0.2907\nEpoch 32 | Train Loss: 0.2408 | Val Loss: 0.2110 | RMSE: 0.4594 | MAE: 0.2990\nEpoch 33 | Train Loss: 0.2339 | Val Loss: 0.2090 | RMSE: 0.4572 | MAE: 0.2988\nEpoch 34 | Train Loss: 0.2313 | Val Loss: 0.2063 | RMSE: 0.4542 | MAE: 0.2929\nEpoch 35 | Train Loss: 0.2250 | Val Loss: 0.2057 | RMSE: 0.4535 | MAE: 0.2944\nEpoch 36 | Train Loss: 0.2234 | Val Loss: 0.2045 | RMSE: 0.4522 | MAE: 0.2942\nEpoch 37 | Train Loss: 0.2199 | Val Loss: 0.2040 | RMSE: 0.4517 | MAE: 0.2947\nEpoch 38 | Train Loss: 0.2174 | Val Loss: 0.2043 | RMSE: 0.4520 | MAE: 0.2948\nEpoch 39 | Train Loss: 0.2149 | Val Loss: 0.2122 | RMSE: 0.4607 | MAE: 0.3095\nEpoch 40 | Train Loss: 0.2071 | Val Loss: 0.2025 | RMSE: 0.4500 | MAE: 0.3041\nEpoch 41 | Train Loss: 0.2088 | Val Loss: 0.2018 | RMSE: 0.4492 | MAE: 0.3040\nEpoch 42 | Train Loss: 0.2003 | Val Loss: 0.2031 | RMSE: 0.4506 | MAE: 0.3109\nEpoch 43 | Train Loss: 0.1975 | Val Loss: 0.2048 | RMSE: 0.4526 | MAE: 0.3041\nEpoch 44 | Train Loss: 0.2003 | Val Loss: 0.2027 | RMSE: 0.4502 | MAE: 0.3012\nEpoch 45 | Train Loss: 0.1901 | Val Loss: 0.2002 | RMSE: 0.4475 | MAE: 0.3122\nEpoch 46 | Train Loss: 0.1938 | Val Loss: 0.1979 | RMSE: 0.4448 | MAE: 0.3067\nEpoch 47 | Train Loss: 0.1857 | Val Loss: 0.2005 | RMSE: 0.4478 | MAE: 0.3171\nEpoch 48 | Train Loss: 0.1809 | Val Loss: 0.2015 | RMSE: 0.4488 | MAE: 0.3244\nEpoch 49 | Train Loss: 0.1800 | Val Loss: 0.1980 | RMSE: 0.4450 | MAE: 0.3084\nEpoch 50 | Train Loss: 0.1760 | Val Loss: 0.2004 | RMSE: 0.4476 | MAE: 0.3131\nEpoch 51 | Train Loss: 0.1649 | Val Loss: 0.2050 | RMSE: 0.4528 | MAE: 0.3394\nEpoch 52 | Train Loss: 0.1686 | Val Loss: 0.2130 | RMSE: 0.4616 | MAE: 0.3154\nEpoch 53 | Train Loss: 0.1647 | Val Loss: 0.2009 | RMSE: 0.4482 | MAE: 0.3101\nEpoch 54 | Train Loss: 0.1592 | Val Loss: 0.2086 | RMSE: 0.4567 | MAE: 0.3137\nEpoch 55 | Train Loss: 0.1566 | Val Loss: 0.1996 | RMSE: 0.4468 | MAE: 0.3171\nEpoch 56 | Train Loss: 0.1508 | Val Loss: 0.1984 | RMSE: 0.4454 | MAE: 0.3212\nEarly stopping triggered.\nFold 5 final RMSE: 0.4448, MAE: 0.3067\n\nAverage RMSE: 0.4838 ± 0.0784\nAverage MAE: 0.3557 ± 0.0505\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### 7) Evaluate","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntarget = pd.read_csv(\"/kaggle/input/private-hen-productivity-target-labels/production_net.csv\", sep=\";\")\neggs_per_day_df = target[[\"date\", \"Laying.rate....\"]].rename(columns={\"Laying.rate....\": \"productivity rate\"})\neggs_per_day_df['productivity rate'] = eggs_per_day_df['productivity rate'].str.replace(',', '.').astype(float)\neggs_per_day_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:47:08.338811Z","iopub.execute_input":"2025-07-02T10:47:08.339036Z","iopub.status.idle":"2025-07-02T10:47:08.394897Z","shell.execute_reply.started":"2025-07-02T10:47:08.339019Z","shell.execute_reply":"2025-07-02T10:47:08.394282Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"         date  productivity rate\n0  2017-02-27          94.960274\n1  2017-03-01          94.260145\n2  2017-04-25          92.655367\n3  2017-07-12          89.730444\n4  2017-07-22          88.271474","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>productivity rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-02-27</td>\n      <td>94.960274</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-03-01</td>\n      <td>94.260145</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-04-25</td>\n      <td>92.655367</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-07-12</td>\n      <td>89.730444</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-07-22</td>\n      <td>88.271474</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"mean_eggs = eggs_per_day_df[\"productivity rate\"].mean()\nstd_eggs = eggs_per_day_df[\"productivity rate\"].std()\n\nprint(f\"Mean productivity rate: {mean_eggs:.2f}\")\nprint(f\"Standard deviation of productivity rate: {std_eggs:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T10:47:08.395637Z","iopub.execute_input":"2025-07-02T10:47:08.395898Z","iopub.status.idle":"2025-07-02T10:47:08.400661Z","shell.execute_reply.started":"2025-07-02T10:47:08.395873Z","shell.execute_reply":"2025-07-02T10:47:08.399969Z"}},"outputs":[{"name":"stdout","text":"Mean productivity rate: 87.16\nStandard deviation of productivity rate: 8.34\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### Model Evaluation: Egg Productivity Prediction\n\n#### Baseline Statistics\n- **Mean productivity rate**: 87.16  \n- **Standard deviation**: 8.34  \n\n---\n\n#### Model 1: Global Mean Pooling\n- **Average RMSE**: 0.8905 ± 0.3084  \n- **Average MAE**: 0.7193 ± 0.2433  \n- **Analysis**:  \n  Highest error among all models. The high MAE and RMSE suggest that mean pooling fails to capture meaningful variation between nodes, likely due to oversimplification of node-level features.\n\n---\n\n#### Model 2: Global Max Pooling\n- **Average RMSE**: 0.8453 ± 0.1600  \n- **Average MAE**: 0.6939 ± 0.1267  \n- **Analysis**:  \n  Slight improvement over mean pooling, particularly in RMSE. Max pooling may help highlight dominant node signals, but still shows weak overall performance.\n\n---\n\n#### Model 3: Global Attention Pooling\n- **Average RMSE**: 0.4838 ± 0.0784  \n- **Average MAE**: 0.3557 ± 0.0505  \n- **Analysis**:  \n  Best performance overall. The attention mechanism helps the model focus on the most relevant nodes, improving accuracy and reducing error. MAE (0.3557) is roughly 1/23rd of the standard deviation, indicating strong predictive performance relative to data variability.\n\n---\n\n#### Overall Insights\n- Switching from mean or max pooling to attention pooling substantially reduced prediction error.  \n- Attention pooling outperformed both mean and max in terms of both RMSE and MAE.  \n- The lower standard deviation in model 3's error metrics suggests more consistent predictions across samples.\n- The MAE (0.3557) of attention pooling is roughly **1/23rd** of the standard deviation, indicating a strong predictive performance relative to data variability.","metadata":{}}]}
